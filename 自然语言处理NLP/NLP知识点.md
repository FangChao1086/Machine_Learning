# NLP知识点
* [词袋模型](#词袋模型)
* [TF-IDF](#TF-IDF)
* [Word2vec](#Word2vec)

<span id="词袋模型"></span>
## 词袋模型
词袋模型：在词集的基础上如果一个单词在文档中出现不止一次，**统计其出现的次数（频数）**。

<span id="TF-IDF"></span>
## TF-IDF
TF-IDF:一种统计方法，用以评估某一字词对于一个文件集或一个语料库的重要程度。
* 字词的重要性**随着它在文件中出现的次数成正比增加**，但同时会**随着它在语料库中出现的频率成反比下降**

<span id="Word2vec"></span>
## Word2vec
[参考链接:通俗理解word2vec](https://www.jianshu.com/p/471d9bfbd72f)  
* Continuous Bag of Words  
![CBOW](https://i.ibb.co/WWtHKpr/CBOW.png)
输入:某一个特征词的**上下文**相关的词对应的词向量  
输出:这一个**特定词**的词向量  
适用于小型数据库
  * 我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
  * 然后，我们**将上下文单词输入到神经网络，并预测该上下文中心的单词**。
  * 当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。
  * 我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

* Skip-Gram  
![Skip-Gram ](https://i.ibb.co/5xNhS5g/Skip-Gram.png)
输入：**特定词**的词向量  
输出：特定词对应的**上下文**词向量  
适用于大型语料
  * 我们考虑一个包含k个连续项的上下文窗口。
  * 然后，我们**跳过其中一个单词，尝试学习一个神经网络**，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
  * 因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量
