# NLP知识点
* [词袋模型](#词袋模型)
* [TF-IDF](#TF-IDF)
* [Word2vec](#Word2vec)
* [TextRank关键词提取](#TextRank关键词提取)
* [LDA模型](#LDA模型)

<span id="词袋模型"></span>
## 词袋模型
词袋模型：在词集的基础上如果一个单词在文档中出现不止一次，**统计其出现的次数（频数）**。

<span id="TF-IDF"></span>
## TF-IDF
TF-IDF:一种统计方法，用以评估某一字词对于一个文件集或一个语料库的重要程度。
* 字词的重要性**随着它在文件中出现的次数成正比增加**，但同时会**随着它在语料库中出现的频率成反比下降**

<span id="Word2vec"></span>
## Word2vec
[参考链接:通俗理解word2vec](https://www.jianshu.com/p/471d9bfbd72f)  
* Continuous Bag of Words  
![CBOW](https://i.ibb.co/WWtHKpr/CBOW.png)  
输入:某一个特征词的**上下文**相关的词对应的词向量  
输出:这一个**特定词**的词向量  
适用于小型数据库
  * 我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
  * 然后，我们**将上下文单词输入到神经网络，并预测该上下文中心的单词**。
  * 当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。
  * 我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

* Skip-Gram  
![Skip-Gram ](https://i.ibb.co/5xNhS5g/Skip-Gram.png)  
输入：**特定词**的词向量  
输出：特定词对应的**上下文**词向量  
适用于大型语料
  * 我们考虑一个包含k个连续项的上下文窗口。
  * 然后，我们**跳过其中一个单词，尝试学习一个神经网络**，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
  * 因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量

<span id="TextRank"></span>
## TextRank关键词提取
* 如果一个单词出现在很多单词后面的话，那么说明这个单词比较重要
* 一个TextRank值很高的单词后面跟着的一个单词，那么这个单词的TextRank值会相应地因此而提高
  * TextRank中一个单词i的权重取决于与**在i前面的各个点j组成的(i,j)这条边的权重**，以及**j这个点到其他边的权重之和**。
  
**jieba分词TextRank**
* 对每个句子进行分词和词性标注处理
* 过滤掉除指定词性外的其他单词，过滤掉出现在停用词表的单词，过滤掉长度小于2的单词
* 将剩下的单词中循环选择一个单词，将其与其后面4个单词分别组合成4条边。
> 例如：         ['有','媒体', '曝光','高圆圆', '和', '赵又廷','现身', '台北', '桃园','机场','的', '照片']  
  对于‘媒体‘这个单词，就有（'媒体', '曝光'）、（'媒体', '圆'）、（'媒体', '和'）、（'媒体', '赵又廷'）4条边，且每条边权值为1，当这条边在之后再次出现时，权值再在基础上加1.
  
* 有了这些数据后，我们就可以构建出候选关键词图
* 这样我们就可以套用TextRank的公式，迭代传播各节点的权值，直至收敛。
* 对结果中的Rank值进行倒序排序，筛选出前面的几个单词，就是我们需要的关键词了。

<span id="LDA模型"></span>
## LDA模型
* LDA就是在pLSA的基础上加层贝叶斯框架，即LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。
* LDA在PLSA的基础上，为**主题分布和词分布**分别加了两个Dirichlet先验。  
![LDA_pLSA](https://i.ibb.co/3rrSbVg/LDA-p-LSA-png.jpg)  
在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。同LDA  
* PLSA中，**主题分布和词分布是唯一确定的**，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}  
![PLSA](https://i.ibb.co/3mSRDDk/LDA-png.jpg)  
* LDA中，**主题分布和词分布不再唯一确定不变**，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即**主题分布跟词分布由Dirichlet先验随机确定**。  
![LDA](https://i.ibb.co/h22s6Hq/PLSA-png.jpg)  

