# NLP知识点
* [词袋模型](#词袋模型)
* [TF-IDF](#TF-IDF)
* [Word2vec](#Word2vec)

<span id="词袋模型"></span>
## 词袋模型
词袋模型：在词集的基础上如果一个单词在文档中出现不止一次，**统计其出现的次数（频数）**。

<span id="TF-IDF"></span>
## TF-IDF
TF-IDF:一种统计方法，用以评估某一字词对于一个文件集或一个语料库的重要程度。
* 字词的重要性**随着它在文件中出现的次数成正比增加**，但同时会**随着它在语料库中出现的频率成反比下降**

<span id="Word2vec"></span>
## Word2vec
![Word2vec](https://i.ibb.co/6WTHcdT/Word2vec.png)
* Continuous Bag of Words  
  * 我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
  * 然后，我们**将上下文单词输入到神经网络，并预测该上下文中心的单词**。
  * 当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。
  * 我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

* Skip-Gram  
  * 我们考虑一个包含k个连续项的上下文窗口。
  * 然后，我们**跳过其中一个单词，尝试学习一个神经网络**，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
  * 因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量
