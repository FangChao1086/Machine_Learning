# NLP知识点
* [词袋模型](#词袋模型)
* [TF-IDF](#TF-IDF)
* [Word2vec](#Word2vec)
* [TextRank关键词提取](#TextRank关键词提取)
* [LDA模型](#LDA模型)
* [余弦相似度算法计算文本相似度 ](#余弦相似度算法计算文本相似度)
* [simhash](#simhash)
* [Elasticsearch](#Elasticsearch)
* [文本分析](#文本分析)
  * [词性标注](#词性标注)
* [jieba分词](#jieba分词)
* [Seq2Seq](#Seq2Seq)
* [实现多轮对话](#实现多轮对话)

<span id="词袋模型"></span>
## 词袋模型
词袋模型：在词集的基础上如果一个单词在文档中出现不止一次，**统计其出现的次数（频数）**。

<span id="TF-IDF"></span>
## TF-IDF
TF-IDF:一种统计方法，用以评估某一字词对于一个文件集或一个语料库的重要程度。
* 字词的重要性**随着它在文件中出现的次数成正比增加**，但同时会**随着它在语料库中出现的频率成反比下降**

<span id="Word2vec"></span>
## Word2vec
[参考链接:通俗理解word2vec](https://www.jianshu.com/p/471d9bfbd72f)  
* Continuous Bag of Words  
![CBOW](https://i.ibb.co/WWtHKpr/CBOW.png)  
输入:某一个特征词的**上下文**相关的词对应的词向量  
输出:这一个**特定词**的词向量  
适用于小型数据库
  * 我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
  * 然后，我们**将上下文单词输入到神经网络，并预测该上下文中心的单词**。
  * 当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。
  * 我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

* Skip-Gram  
![Skip-Gram ](https://i.ibb.co/5xNhS5g/Skip-Gram.png)  
输入：**特定词**的词向量  
输出：特定词对应的**上下文**词向量  
适用于大型语料
  * 我们考虑一个包含k个连续项的上下文窗口。
  * 然后，我们**跳过其中一个单词，尝试学习一个神经网络**，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
  * 因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量

<span id="TextRank关键词提取"></span>
## TextRank关键词提取
* 如果一个单词出现在很多单词后面的话，那么说明这个单词比较重要
* 一个TextRank值很高的单词后面跟着的一个单词，那么这个单词的TextRank值会相应地因此而提高
  * TextRank中一个单词i的权重取决于与**在i前面的各个点j组成的(i,j)这条边的权重**，以及**j这个点到其他边的权重之和**。
  
**jieba分词TextRank**
* 对每个句子进行分词和词性标注处理
* 过滤掉除指定词性外的其他单词，过滤掉出现在停用词表的单词，过滤掉长度小于2的单词
* 将剩下的单词中循环选择一个单词，将其与其后面4个单词分别组合成4条边。
> 例如：         ['有','媒体', '曝光','高圆圆', '和', '赵又廷','现身', '台北', '桃园','机场','的', '照片']  
  对于‘媒体‘这个单词，就有（'媒体', '曝光'）、（'媒体', '圆'）、（'媒体', '和'）、（'媒体', '赵又廷'）4条边，且每条边权值为1，当这条边在之后再次出现时，权值再在基础上加1.
  
* 有了这些数据后，我们就可以构建出候选关键词图
* 这样我们就可以套用TextRank的公式，迭代传播各节点的权值，直至收敛。
* 对结果中的Rank值进行倒序排序，筛选出前面的几个单词，就是我们需要的关键词了。

<span id="LDA模型"></span>
## LDA模型
* LDA就是在pLSA的基础上加层贝叶斯框架，即LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。
* LDA在PLSA的基础上，为**主题分布和词分布**分别加了两个Dirichlet先验。  

**beta分布是二项式分布的共轭先验概率分布，Dirichlet分布是多项式分布的共轭先验概率分布**  

![LDA_pLSA](https://i.ibb.co/3rrSbVg/LDA-p-LSA-png.jpg)  

在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。同LDA  

![PLSA](https://i.ibb.co/h22s6Hq/PLSA-png.jpg)  

* PLSA中，**主题分布和词分布是唯一确定的**，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}  

![LDA](https://i.ibb.co/3mSRDDk/LDA-png.jpg)  

* LDA中，**主题分布和词分布不再唯一确定不变**，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即**主题分布跟词分布由Dirichlet先验随机确定**。  

### LDA模型中一篇文档生成的方式如下:
* 从狄利克雷分布\alpha中取样生成文档i的主题分布\theta_i
* 从主题的多项式分布\theta_i中取样生成文档i第j个词的主题z_{i, j}
* 从狄利克雷分布\beta中取样生成主题z_{i, j}的词语分布\phi_{z_{i, j}}
* 从词语的多项式分布\phi_{z_{i, j}}中采样最终生成词语w_{i, j}

<span id="余弦相似度算法计算文本相似度"></span>
## 余弦相似度算法计算文本相似度 
用于文本匹配，精确度较高  
[参考链接：余弦相似度算法计算文本相似度](https://www.cnblogs.com/airnew/p/9563703.html)

<span id="simhash"></span>
## simhash
* 局部敏感散列（locality sensitive hash）的一种，
* 主要思想是降维,将高维的特征向量映射成低维的特征向量，通过两个向量的**汉明距离**来判定文章是否重复或者高度近似  
* 在信息论中，两个等长字符串之间的**汉明距离**是两个字符串对应位置的不同字符的个数。也就是说，它就是将**一个字符串变换成另外一个字符串需要替换的字符个数**。例如，1011101与1001001之间的汉明距离是2。**两个二进制数“异或”后得到1的个数即为汉明距离的大小**  
### simhash算法流程
（1）分词。  
> 给定一段语句，进行分词，得到有效的特征向量，然后为每一个特征向量设置1～5这5个级别的权（如果是给定一个文本，那么特征向量可以是文本中的词，其权可以是这个词出现的次数）。
>> 例如，给定一段语句：“CSDN博客结构之法算法之道的作者July。”分词后为：“CSDN/博客/结构/之/法/算法/之/道/的/作者/July。”然后为每个特征向量赋予权值：CSDN(4)/博客(5)/结构(3)/之(1)/法(2)/算法(3)/之(1)/道(2)/的(1)/作者(5)/July(5)。其中括号里的数字代表这个单词在整条语句中的重要程度，数字越大代表越重要。

（2）散列。  
> 通过散列函数计算各个特征向量的散列值，散列值为二进制数0和1组成的n位签名。  
>> 比如“CSDN”的散列值hash(CSDN)为[1 0 0 10 1]，“博客”的散列值hash（博客）为[1 0 1 0 1 1]。就这样，字符串就变成了一系列数字。

（3）加权。  
> 在散列值的基础上，给所有特征向量加权，即W = hash×weight，且遇到1则让散列值和权值正相乘，遇到0则让散列值和权值负相乘。  
>> 例如，给“CSDN”的散列值“1 0 0 1 0 1”加权得到W(CSDN) = [1 0 0 1 0 1] × 4 = [4 −4 −4 4 −4 4]，给“博客”的散列值[1 0 1 0 1 1]加权得到W（博客）=[1 0 1 0 1 1] × 5 = [5 −5 5 −5 5 5]，其余特征向量的操作与此类似。

（4）合并。  
> 将上述各个特征向量的加权结果累加，变成只有一个序列串。仅拿前两个特征向量举例  
>> 例如“CSDN”的[4 −4 −4 4 −4 4]和“博客”的[5 −5 5 −5 5 5]进行累加，得到[4+5 (−4)+(−5) (−4)+5 4+(−5) (−4)+5 4+5]，得到[9 −9 1 −1 1 9]。

（5）降维。  
> 如果加权的n位签名的累加结果大于0则置1，否则置0，从而得到该语句的simhash值。  
>> 例如，把上面计算出来的[9 −9 1 −1 1 9]降维，得到的01串为“1 0 1 0 1 1”，从而形成“CSDN博客”这个短语的simhash签名。最后便可以根据不同语句的simhash的汉明距离来判断它们的相似度。  

![simhash](https://i.ibb.co/7C4Vwdv/simhash.png)  
**得到了每篇文档的simhash签名值后，只要计算两个签名的汉明距离即可判断它们之间的相似度**  

扩展到海量数据：
* 我们可以把64位的二进制simhash签名均分成4块，每块16位。根据鸽巢原理（也称抽屉原理），如果两个签名的汉明距离在3以内，它们必有一块完全相同，


<span id="Elasticsearch"></span>
## Elasticsearch
* 搜索引擎数据库，可以加快搜索匹配过程  
倒排索引:搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。倒排索引是一种像数据结构一样的散列图，可将用户从单词导向文档或网页。它是搜索引擎的核心。其主要目标是快速搜索从数百万文件中查找数据

<span id="文本分析"></span>
## 文本分析

<span id="词性标注"></span>
## 词性标注
**1. 基于字符串匹配的字典查找算法**
* 先对语句进行**分词**，然后从**字典中查找每个词语的词性**，对其**进行标注**即可。
* jieba词性标注中，对于识别出来的词语，就是采用了这种方法。
* 这种方法比较简单，通俗易懂，但是不能解决一词多词性的问题，因此存在一定的误差。

**2. 基于统计的词性标注算法**
* 和分词一样，我们也可以通过HMM隐马尔科夫模型来进行词性标注。
* **观测序列即为分词后的语句，隐藏序列即为经过标注后的词性标注序列**。
* 观测序列到隐藏序列的计算可以通过viterbi算法，利用统计得到的起始概率、发射概率和转移概率来得到。
  * 起始概率、发射概率和转移概率和分词中的含义大同小异，可以通过大规模语料统计得到。
* 得到隐藏序列后，就完成了词性标注过程。

<span id="jieba分词"></span>
## jieba分词
[参考链接：对Python中文分词模块结巴分词算法过程的理解和分析](https://blog.csdn.net/rav009/article/details/12196623)  
[参考链接：结巴分词原理介绍](https://blog.csdn.net/baidu_33718858/article/details/81073093)

### 结巴分词用到的算法
* 基于**Trie树**结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的**有向无环图（DAG)**
* 采用了动态规划**查找最大概率路径**, 找出基于词频的最大切分组合
* 对于**未登录词**，采用了基于汉字成词能力的HMM模型，**使用了Viterbi算法**


jieba分词综合了基于字符串匹配的算法和基于统计的算法  
其分词步骤为:  
1. 初始化。  
> 加载词典文件，获取每个词语和它出现的词数
2. 切分短语。  
> 利用正则，将文本切分为一个个语句，之后对语句进行分词
3. 构建DAG。  
> 通过字符串匹配，构建所有可能的分词情况的有向无环图，也就是DAG
4. 构建节点最大路径概率，以及结束位置。  
> 计算每个汉字节点到语句结尾的所有路径中的最大概率，并记下最大概率时在DAG中对应的该汉字成词的结束位置。
5. 构建切分组合。  
> 根据节点路径，得到词语切分的结果，也就是分词结果。
6. HMM新词处理  
> 对于新词，也就是dict.txt中没有的词语，我们通过统计方法来处理，jieba中采用了HMM隐马尔科夫模型来处理。
7. 返回分词结果  
> 通过yield将上面步骤中切分好的词语逐个返回。yield相对于list，可以节约存储空间。

<span id="Seq2Seq"></span>
## Seq2Seq
[参考链接：Seq2Seq](https://blog.csdn.net/dcrmg/article/details/80327069)

<span id="实现多轮对话"></span>
## 实现多轮对话
* 首先**对用户的输入进行分类**（专业话题or闲聊话题），
* 根据分类结果不同**匹配不同的库**
* 然后向用户**返回不同的结果**
* 根据MDP（**马尔可夫决策过程**）实现多轮对话
