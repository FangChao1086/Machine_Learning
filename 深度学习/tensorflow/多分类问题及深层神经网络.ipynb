{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST手写体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MNIST`手写体识别是图像识别中最经典的问题, 希望能够识别出人类手写的数字. 数据是65000张灰度图和对应的数字. 我们用之前的深度神经网络来尝试解决这个问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow 已经把 mnist 数据集集成在 examples 里面了\n",
    "# 在这里 import 数据输入的部分\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们导入`mnist`数据集\n",
    "\n",
    "**注意** 下载数据需要一定时间, 如果下面这行代码下载出现问题, 可以从网上下载 MNIST 数据集, 然后一起放到`MNIST_data`这个文件夹中, 文件夹的结构应该是下面这样:\n",
    "```\n",
    "    MNIST_data\n",
    "        train-images-idx3-ubyte.gz\n",
    "        train-labels-idx1-ubyte.gz\n",
    "        t10k-images-idx3-ubyte.gz\n",
    "        t10k-labels-idx1-ubyte.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-1390ba3ba838>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\D\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\D\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\D\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\D\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\D\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到`read_data_sets`的一个参数是**`one_hot`**([wiki](https://en.wikipedia.org/wiki/One-hot)). \n",
    "\n",
    "它是识别任务中非常重要的一个概念, 将一个数值`n`映射到一个向量, 这个向量的第`n`个元素是`1`, 其他元素都是`0`.\n",
    "\n",
    "这个数据集分成了两个部分: 训练和测试. 分开来是为了观察模型在完全没见过的数据上的表现, 从而体现泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们具体看看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAACwCAYAAAAVHflSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(ncols=6, nrows=2)\n",
    "plt.tight_layout(w_pad=-2.0, h_pad=-8.0)\n",
    "\n",
    "# 调用next_batch方法来一次性获取12个样本,\n",
    "# 这里有一个`shuffle`参数, 表达是否打乱样本间的顺序\n",
    "images, labels = train_set.next_batch(12, shuffle=False)\n",
    "\n",
    "for ind, (image, label) in enumerate(zip(images, labels)):\n",
    "    # image 是一个 784 维的向量, 是图片进行拉伸产生的, 这里我们给它 reshape 回去\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    # label 是一个 10 维的向量, 哪个下标处的值为1 说明是数字几\n",
    "    label = label.argmax()\n",
    "\n",
    "    row = ind // 6\n",
    "    col = ind % 6\n",
    "    axes[row][col].imshow(image, cmap='gray') # 灰度图\n",
    "    axes[row][col].axis('off')\n",
    "    axes[row][col].set_title('%d' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义深度网络结构."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hidden_layer(layer_input, output_depth, scope='hidden_layer', reuse=None):\n",
    "    input_depth = layer_input.get_shape()[-1]\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 注意这里的初始化方法是truncated_normal\n",
    "        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), shape=(input_depth, output_depth), name='weights')\n",
    "        # 注意这里用 0.1 对偏置进行初始化\n",
    "        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name='bias')\n",
    "        net = tf.matmul(layer_input, w) + b\n",
    "        \n",
    "        return net\n",
    "\n",
    "def DNN(x, output_depths, scope='DNN', reuse=None):\n",
    "    net = x\n",
    "    for i, output_depth in enumerate(output_depths):\n",
    "        net = hidden_layer(net, output_depth, scope='layer%d' % i, reuse=reuse)\n",
    "        # 注意这里的激活函数\n",
    "        net = tf.nn.relu(net)\n",
    "    # 数字分为0, 1, ..., 9 所以这是10分类问题\n",
    "    # 对应于 one_hot 的标签, 所以这里输出一个 10维 的向量\n",
    "    net = hidden_layer(net, 10, scope='classification', reuse=reuse)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个4层的神经网络, 它的隐藏节点数分别为: 400, 200, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这是一个分类问题, 因此我们采用交叉熵来计算损失函数\n",
    "loss = tf.losses.softmax_cross_entropy(logits=dnn, onehot_labels=label_ph)\n",
    "\n",
    "# 下面定义的是正确率, 注意理解它为什么是这么定义的\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.508236 train_acc: 0.859375 test_loss: 0.381798 test_acc: 0.890625\n",
      "STEP 2000: train_loss: 0.273473 train_acc: 0.906250 test_loss: 0.308054 test_acc: 0.937500\n",
      "STEP 3000: train_loss: 0.300518 train_acc: 0.921875 test_loss: 0.129032 test_acc: 0.968750\n",
      "STEP 4000: train_loss: 0.287986 train_acc: 0.906250 test_loss: 0.137869 test_acc: 0.953125\n",
      "STEP 5000: train_loss: 0.158539 train_acc: 0.968750 test_loss: 0.343361 test_acc: 0.921875\n",
      "STEP 6000: train_loss: 0.049023 train_acc: 0.984375 test_loss: 0.233824 test_acc: 0.937500\n",
      "STEP 7000: train_loss: 0.147848 train_acc: 0.953125 test_loss: 0.145326 test_acc: 0.937500\n",
      "STEP 8000: train_loss: 0.168618 train_acc: 0.953125 test_loss: 0.194900 test_acc: 0.921875\n",
      "STEP 9000: train_loss: 0.076882 train_acc: 0.968750 test_loss: 0.126064 test_acc: 0.968750\n",
      "STEP 10000: train_loss: 0.121944 train_acc: 0.984375 test_loss: 0.106106 test_acc: 0.937500\n",
      "STEP 11000: train_loss: 0.078532 train_acc: 0.984375 test_loss: 0.070194 test_acc: 0.968750\n",
      "STEP 12000: train_loss: 0.137155 train_acc: 0.968750 test_loss: 0.040063 test_acc: 1.000000\n",
      "STEP 13000: train_loss: 0.082788 train_acc: 0.953125 test_loss: 0.167598 test_acc: 0.953125\n",
      "STEP 14000: train_loss: 0.046237 train_acc: 0.984375 test_loss: 0.252425 test_acc: 0.921875\n",
      "STEP 15000: train_loss: 0.054545 train_acc: 0.984375 test_loss: 0.075183 test_acc: 0.968750\n",
      "STEP 16000: train_loss: 0.060618 train_acc: 0.968750 test_loss: 0.062675 test_acc: 0.968750\n",
      "STEP 17000: train_loss: 0.015194 train_acc: 1.000000 test_loss: 0.161081 test_acc: 0.937500\n",
      "STEP 18000: train_loss: 0.004236 train_acc: 1.000000 test_loss: 0.125630 test_acc: 0.968750\n",
      "STEP 19000: train_loss: 0.044804 train_acc: 0.984375 test_loss: 0.124513 test_acc: 0.968750\n",
      "STEP 20000: train_loss: 0.033218 train_acc: 1.000000 test_loss: 0.160254 test_acc: 0.953125\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.049369\n",
      "Train accuracy: 0.986836\n",
      "Test loss: 0.090361\n",
      "Test accuracy: 0.970600\n"
     ]
    }
   ],
   "source": [
    "# 我们训练20000次\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    # 获取 batch_size个训练样本\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        # 获取 batch_size 个测试样本\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 计算在当前样本上的训练以及测试样本的损失值和正确率\n",
    "        loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 最后在训练集上我们达到了大约0.98的正确率, 在测试集上我们达到了大约0.97的正确率, 已经是一个相当不错的成绩了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard & `tf.summary`\n",
    "\n",
    "[前面](https://github.com/SherlockLiao/ai-class-intro/blob/master/tensorflow/course_0/tensorflow-basic.ipynb)已经给大家分享过如何使用`Tensorboard`将我们构建的计算图显示出来, 这里我们还要介绍它和`tf.summary`结合起来体现的更加强大的功能: 可视化训练. \n",
    "\n",
    "首先介绍一下`tf.summary`, 它能够收集训练过程中的各种`tensor`的信息并把它保存起来以供`Tensorboard`读取并展示. 按照下面的方法来使用它:\n",
    "\n",
    "### 构造`summary`\n",
    "- - -\n",
    "- 如果你想收集表示一个标量或者一个数的`tensor`的信息, 比如上面的`loss`\n",
    "```python\n",
    "loss_sum = tf.summary.scalar('loss', loss)\n",
    "```\n",
    "上面的语句就会告诉`Tensorflow`, 在运行过程中, 我要让`Tensorboard`显示误差的变化了\n",
    "- - -\n",
    "- 如果你想收集一个`tensor`的分布情况, 这个`tensor`可以是任意形状, 比如我们定义了一个`(784, 400)`的权重`w`\n",
    "```python\n",
    "w_hist = tf.summary.histogram('w_hist', w)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个4维的1-通道(灰度图), 3-通道(RGB), 4-通道(RGBA)的`tensor`的变化, 比如我们输出了一个`(1, 8, 8, 1)`的灰度图`image`\n",
    "```python\n",
    "image_sum = tf.summary.image('image', image)\n",
    "```\n",
    "- - -\n",
    "- 如果你想收集一个3维(batch, frame, channel), 2维(batch, frame)的变化, 比如我们输出了一个`(10, 50, 3)`的`tensor`:`audio`\n",
    "```python\n",
    "audio_sum = tf.summary.audio('audio', audio)\n",
    "```\n",
    "- - -\n",
    "在这次课程中, 我们暂时先使用`scalar`和`histogram`的`summary`, `image`和`audio`的`summary`将在之后的课程中介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 重新定义占位符\n",
    "input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 我们需要重新构建前向神经网络, 为了简化代码, 我们在构造一个隐藏层以及它的参数的函数内部构造`tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造权重, 用`truncated_normal`初始化\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "# 构造偏置, 用`0.1`初始化\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造添加`variable`的`summary`的函数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算平均值\n",
    "        mean = tf.reduce_mean(var)\n",
    "        # 将平均值添加到`summary`中, 这是一个数值, 所以我们用`tf.summary.scalar`\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        # 计算标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        # 将标准差添加到`summary`中\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        \n",
    "        # 添加最大值,最小值`summary`\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        \n",
    "        # 添加这个变量分布情况的`summary`, 我们希望观察它的分布, 所以用`tf.summary.histogram`\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造一个隐藏层\n",
    "def hidden_layer(x, output_dim, scope='hidden_layer', act = tf.nn.relu, reuse=None):\n",
    "    # 获取输入的`depth`\n",
    "    input_dim = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.name_scope(scope):\n",
    "        with tf.name_scope('weight'):\n",
    "            # 构造`weight`\n",
    "            weight = weight_variable([input_dim, output_dim])\n",
    "            # 添加`weight`的`summary`\n",
    "            variable_summaries(weight)\n",
    "            \n",
    "        with tf.name_scope('bias'):\n",
    "            # 构造`bias`\n",
    "            bias = bias_variable([output_dim])\n",
    "            # 添加`bias`的`summary`\n",
    "            variable_summaries(bias)\n",
    "            \n",
    "        with tf.name_scope('linear'):\n",
    "            # 计算`xw+b`\n",
    "            preact = tf.matmul(x, weight) + bias\n",
    "            # 添加激活层之前输出的分布情况到`summary`\n",
    "            tf.summary.histogram('pre_activation', preact)\n",
    "            \n",
    "        # 经过激活层`act`\n",
    "        output = act(preact)\n",
    "        # 添加激活后输出的分布情况到`summary`\n",
    "        tf.summary.histogram('output', output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造深度神经网络\n",
    "def DNN(x, output_depths, scope='DNN_with_sums', reuse=None):\n",
    "    with tf.name_scope(scope):\n",
    "        net = x\n",
    "        for i, output_depth in enumerate(output_depths):\n",
    "            net = hidden_layer(net, output_depth, scope='hidden%d' % (i + 1), reuse=reuse)\n",
    "        # 最后有一个分类层\n",
    "        net = hidden_layer(net, 10, scope='classification', act=tf.identity, reuse=reuse)\n",
    "        return net\n",
    "\n",
    "dnn_with_sums = DNN(input_ph, [400, 200, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重新定义`loss`, `acc`, `train_op`\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.losses.softmax_cross_entropy(logits=dnn_with_sums, onehot_labels=label_ph)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(dnn_with_sums, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    lr = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (可选)融合`summary`\n",
    "- - -\n",
    "- 我们可以把前面定义的所有`summary`都融合成一个`summary`\n",
    "```python\n",
    "merged = tf.summary.merge_all()\n",
    "```\n",
    "- - -\n",
    "- 也可以只是融合某些`summary`\n",
    "```python\n",
    "merged = tf.summary.merge([loss_sum, image_sum])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出`summary`\n",
    "---\n",
    "`summary`是需要导出到外部文件的\n",
    "- 首先定义一个文件读写器\n",
    "```python\n",
    "summary_writer = tf.summary.FileWriter('summaries', sess.graph)\n",
    "```\n",
    "- - -\n",
    "- 然后在训练的过程中, 在你希望的时候运行一次`merged`或者是你之前自己定义的某个通过`summary`定义的`op`\n",
    "```python\n",
    "summaries = sess.run(merged, feed_dict={...})\n",
    "```\n",
    "- - -\n",
    "- 然后将这个`summaries`写入到`summari_writer`内\n",
    "```python\n",
    "summary_writer.add_summary(summaries, step)\n",
    "```\n",
    "注意`step`表示你当前训练的步数, 当然你也可以设定为其他你想要用的数值\n",
    "\n",
    "- - -\n",
    "- 最后关闭文件读写器\n",
    "```python\n",
    "summary_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('test_summary/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('test_summary/test', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.450756 train_acc: 0.875000 test_loss: 0.313196 test_acc: 0.921875\n",
      "STEP 2000: train_loss: 0.493976 train_acc: 0.875000 test_loss: 0.338943 test_acc: 0.859375\n",
      "STEP 3000: train_loss: 0.217328 train_acc: 0.937500 test_loss: 0.364094 test_acc: 0.890625\n",
      "STEP 4000: train_loss: 0.073134 train_acc: 0.984375 test_loss: 0.098965 test_acc: 0.984375\n",
      "STEP 5000: train_loss: 0.073675 train_acc: 1.000000 test_loss: 0.048723 test_acc: 1.000000\n",
      "STEP 6000: train_loss: 0.198771 train_acc: 0.953125 test_loss: 0.079934 test_acc: 0.968750\n",
      "STEP 7000: train_loss: 0.021829 train_acc: 1.000000 test_loss: 0.114877 test_acc: 0.953125\n",
      "STEP 8000: train_loss: 0.077190 train_acc: 0.953125 test_loss: 0.107403 test_acc: 0.984375\n",
      "STEP 9000: train_loss: 0.248546 train_acc: 0.937500 test_loss: 0.235702 test_acc: 0.921875\n",
      "STEP 10000: train_loss: 0.023094 train_acc: 1.000000 test_loss: 0.107692 test_acc: 0.968750\n",
      "STEP 11000: train_loss: 0.066894 train_acc: 0.984375 test_loss: 0.059660 test_acc: 0.984375\n",
      "STEP 12000: train_loss: 0.102780 train_acc: 0.953125 test_loss: 0.136072 test_acc: 0.968750\n",
      "STEP 13000: train_loss: 0.142124 train_acc: 0.968750 test_loss: 0.035954 test_acc: 0.984375\n",
      "STEP 14000: train_loss: 0.066630 train_acc: 0.984375 test_loss: 0.081612 test_acc: 0.984375\n",
      "STEP 15000: train_loss: 0.022145 train_acc: 1.000000 test_loss: 0.109468 test_acc: 0.953125\n",
      "STEP 16000: train_loss: 0.053965 train_acc: 0.984375 test_loss: 0.059319 test_acc: 0.968750\n",
      "STEP 17000: train_loss: 0.035579 train_acc: 1.000000 test_loss: 0.044271 test_acc: 0.984375\n",
      "STEP 18000: train_loss: 0.013966 train_acc: 1.000000 test_loss: 0.171629 test_acc: 0.937500\n",
      "STEP 19000: train_loss: 0.022386 train_acc: 1.000000 test_loss: 0.179903 test_acc: 0.953125\n",
      "STEP 20000: train_loss: 0.025183 train_acc: 1.000000 test_loss: 0.034571 test_acc: 1.000000\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.049280\n",
      "Train accuracy: 0.987273\n",
      "Test loss: 0.094284\n",
      "Test accuracy: 0.972000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(20000):\n",
    "    images, labels = train_set.next_batch(batch_size)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels})\n",
    "    if e % 1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(batch_size)\n",
    "        # 获取`train`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_train, loss_train, acc_train = sess.run([merged, loss, acc], feed_dict={input_ph: images, label_ph: labels})\n",
    "        # 将`train`的`summaries`写入到`train_writer`中\n",
    "        train_writer.add_summary(sum_train, e)\n",
    "        # 获取`test`数据的`summaries`以及`loss`, `acc`信息\n",
    "        sum_test, loss_test, acc_test = sess.run([merged, loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels})\n",
    "        # 将`test`的`summaries`写入到`test_writer`中\n",
    "        test_writer.add_summary(sum_test, e)\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "# 关闭读写器\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "# 计算所有训练样本的损失值以及正确率\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "# 计算所有测试样本的损失值以及正确率\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开`Tensorboard`\n",
    "在之前对计算图可视化的时候, 我们用`tensorboard --logdir=.`命令打开过`Tensorboard`显示当前目录下``. 但`Tensorboard`支持打开多个目录下的`.events`文件, 方便我们对比不同模型或者训练和测试之间的差别\n",
    "\n",
    "\n",
    "在`test_summary`目录中输入以下命令\n",
    "```\n",
    "$ tensorboard --logdir=train:train/,test:test/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们就可以看到类似于下面的界面了\n",
    "- **计算图**\n",
    "<img src=\"https://image.ibb.co/fGfMSc/graph.png\" width=\"500\">\n",
    "- - -\n",
    "- **`loss`和`accuracy`**\n",
    "<figure class=\"half\">\n",
    "    <img src=\"https://image.ibb.co/n1bBSc/loss.png\" align=\"left\">\n",
    "    <img src=\"https://image.ibb.co/dHg0Lx/accuracy.png\" align='right'>\n",
    "</figure>\n",
    "- - -\n",
    "- **所有的标量统计**\n",
    "<img src=\"https://image.ibb.co/kVP9DH/scalars.png\" width=\"800\">\n",
    "- - -\n",
    "- **所有的直方图统计**\n",
    "<img src=\"https://image.ibb.co/bx60Lx/histogram.png\" width=\"800\">\n",
    "- - - \n",
    "- **所有的分布统计**\n",
    "<img src=\"https://image.ibb.co/irS70x/distribution.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`有各种各样的有意思的地方, 大家可以多探索探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结语\n",
    "这次大家学习到了如何对`MNIST`手写体识别问题构建深度神经网络模型, 以及如何使用`Tensorboard`对训练过程进行可视化. 在接下来的课程中, 我们将要进入到深度学习的卷积神经网络部分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
