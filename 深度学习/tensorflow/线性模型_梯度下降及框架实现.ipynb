{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042],\n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],\n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827],\n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们来看看数据具体的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后, 把数据转换成`tensorflow`的`tensor`形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(x_train, name='x')\n",
    "y = tf.constant(y_train, name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一个线性模型\n",
    "- 定义模型的`w`以及`b`参数\n",
    "- 用`w, b`定义这个线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(initial_value=tf.random_normal(shape=(), seed=2017), dtype=tf.float32, name='weight')\n",
    "b = tf.Variable(initial_value=0, dtype=tf.float32, name='biase')\n",
    "\n",
    "with tf.variable_scope('Linear_Model'):\n",
    "    y_pred = w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意`tf.variable_scope()`这个函数, 它是用来规定一个变量的`区域`的, 在这个`with`语句下定义的所有变量都在同一个`变量域`当中, `域名`就是`variable_scope()`的参数. \n",
    "\n",
    "那么它有什么用呢?\n",
    "\n",
    "实际上, 所有变量域中的变量的名字都以域名为前缀:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w.name)\n",
    "print(y_pred.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开启交互式会话\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 一定要有初始化这一步!!!\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了, 现在我们可以看一下这个线性模型的输出具体长什么样了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 要先将`tensor`的内容`fetch`出来\n",
    "y_pred_numpy = y_pred.eval(session=sess)\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo', label='real')\n",
    "plt.plot(x_train, y_pred_numpy, 'ro', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型\n",
    "\n",
    "- 定义误差函数\n",
    "\n",
    "前面提到过, 为了优化我们的模型, 需要构建一个误差(`loss`)函数, 来告诉我们优化的好坏程度.\n",
    "\n",
    "而这里, 我们想要预测值和真实值尽可能接近, 因此, 我们就用上面定义的`loss`进行衡量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "\n",
    "# 看看在当前模型下的误差有多少\n",
    "print(loss.eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们用梯度下降法去优化这个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们需要求解误差函数对于每个参数的梯度. 通过求导知识可以知道是下面的形式:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^n x_i(w x_i + b - y_i) \\\\\n",
    "\\frac{\\partial}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i)\n",
    "$$\n",
    "\n",
    "但实际上我们并不会这么去用, 因为`tensorflow`拥有自动求导功能, 省去了这些数学知识以及手工求导的复杂工作. 可以通过下面的代码去获得一个标量对参数的导函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grad, b_grad = tf.gradients(loss, [w, b])\n",
    "\n",
    "print('w_grad: %.4f' % w_grad.eval(session=sess))\n",
    "print('b_grad: %.4f' % b_grad.eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对梯度乘上一个`步长(lr)`来更新参数.一般我们把这个步长称为学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "w_update = w.assign_sub(lr * w_grad)\n",
    "b_update = b.assign_sub(lr * b_grad)\n",
    "\n",
    "sess.run([w_update, b_update])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在更新参数完成后, 我们再一次看看模型的输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "y_pred_numpy = y_pred.eval(session=sess)\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo', label='real')\n",
    "plt.plot(x_train, y_pred_numpy, 'ro', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新一次之后, 我们发现红色点跑到了蓝色点附近, 相比之前靠得更近了, 说明通过梯度下降模型得到了优化. 当然我们可以多更新几次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(10):\n",
    "    sess.run([w_update, b_update])\n",
    "    \n",
    "    y_pred_numpy = y_pred.eval(session=sess)\n",
    "    loss_numpy = loss.eval(session=sess)\n",
    "    \n",
    "    ax.clear()\n",
    "    ax.plot(x_train, y_train, 'bo', label='real')\n",
    "    ax.plot(x_train, y_pred_numpy, 'ro', label='estimated')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()\n",
    "    plt.pause(0.5)\n",
    "    \n",
    "    print('epoch: {}, loss: {}'.format(e, loss_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看看最后的模型结果吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(x_train, y_train, 'bo', label='real')\n",
    "plt.plot(x_train, y_pred_numpy, 'ro', label='estimated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过 10 次更新，我们发现红色的预测结果已经比较好的拟合了蓝色的真实值。\n",
    "\n",
    "现在你已经学会了你的第一个机器学习模型了，再接再厉，完成下面的小练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：**\n",
    "\n",
    "重启 notebook 运行上面的线性回归模型，但是改变训练次数以及不同的学习率进行尝试得到不同的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将之前的`graph`清除\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多变量函数\n",
    "\n",
    "w_target = np.array([0.5, 3, 2.4]) # 定义参数\n",
    "b_target = np.array([0.9]) # 定义参数\n",
    "\n",
    "f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(\n",
    "    b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的式子\n",
    "\n",
    "print(f_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样地, 我们看看这个多项式的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 画出这个函数的曲线\n",
    "x_sample = np.arange(-3, 3.1, 0.1)\n",
    "y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3\n",
    "\n",
    "plt.plot(x_sample, y_sample, label='real curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们构造形如$[x, x^{2}, x^{3}]$这样的数据, 把多项式回归问题转换为线性回归问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)\n",
    "x_train = tf.constant(x_train, dtype=tf.float32, name='x_train')\n",
    "y_train = tf.constant(y_sample, dtype=tf.float32, name='y_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(initial_value=tf.random_normal(shape=(3, 1)), dtype=tf.float32, name='weights')\n",
    "b = tf.Variable(initial_value=0, dtype=tf.float32, name='bias')\n",
    "\n",
    "def multi_linear(x):\n",
    "    return tf.squeeze(tf.matmul(x, w) + b)\n",
    "\n",
    "y_ = multi_linear(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出模型输出的结果和真实结果的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x_train_value = x_train.eval(session=sess)\n",
    "y_train_value = y_train.eval(session=sess)\n",
    "y_pred_value = y_.eval(session=sess)\n",
    "\n",
    "plt.plot(x_train_value[:,0], y_pred_value, label='fitting curve', color='r')\n",
    "plt.plot(x_train_value[:,0], y_train_value, label='real curve', color='b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样地, 定义`loss`函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y_train - y_))\n",
    "loss_numpy = sess.run(loss)\n",
    "print(loss_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用`tf.gradients()`自动求解导数\n",
    "w_grad, b_grad = tf.gradients(loss, [w, b])\n",
    "\n",
    "print(w_grad.eval(session=sess))\n",
    "print(b_grad.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用梯度下降更新参数\n",
    "lr = 1e-3\n",
    "\n",
    "w_update = w.assign_sub(lr * w_grad)\n",
    "b_update = b.assign_sub(lr * b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看更新一次之后的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sess.run([w_update, b_update])\n",
    "\n",
    "x_train_value = x_train.eval(session=sess)\n",
    "y_train_value = y_train.eval(session=sess)\n",
    "y_pred_value = y_.eval(session=sess)\n",
    "loss_numpy = loss.eval(session=sess)\n",
    "\n",
    "plt.plot(x_train_value[:,0], y_pred_value, label='fitting curve', color='r')\n",
    "plt.plot(x_train_value[:,0], y_train_value, label='real curve', color='b')\n",
    "plt.legend()\n",
    "plt.title('loss: %.4f' % loss_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 一次更新的效果并不好, 那让我们多尝试几次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(100):  \n",
    "    sess.run([w_update, b_update])\n",
    "    \n",
    "    x_train_value = x_train.eval(session=sess)\n",
    "    y_train_value = y_train.eval(session=sess)\n",
    "    y_pred_value = y_.eval(session=sess)\n",
    "    loss_numpy = loss.eval(session=sess)\n",
    "\n",
    "    ax.clear()\n",
    "    ax.plot(x_train_value[:,0], y_pred_value, label='fitting curve', color='r')\n",
    "    ax.plot(x_train_value[:,0], y_train_value, label='real curve', color='b')\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    if (e + 1) % 20 == 0:\n",
    "        print('epoch: {}, loss: {}'.format(e + 1, loss_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，经过 100 次更新之后，可以看到拟合的线和真实的线已经完全重合了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习：上面的例子是一个三次的多项式，尝试使用二次的多项式去拟合它，看看最后能做到多好**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
