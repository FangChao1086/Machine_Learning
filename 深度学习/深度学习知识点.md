# 深度学习知识点
* [反向传播(BP)](#反向传播(BP))
* [LSTM](#LSTM)
* [GRU](#GRU)
* [Dropout](#Dropout)

<span id="反向传播(BP)"></span>
## 反向传播(BP)
[参考链接：手推反向传播](https://www.cnblogs.com/makefile/p/BP.html)
<span id="LSTM"></span>
## LSTM->一种特殊的 RNN 类型
* 设计初衷：避免长期依赖问题，记住长期的信息
* **输入门、遗忘门和输出门来控制输入值、记忆值和输出值**  

![LSTM](https://i.ibb.co/vs4x5sn/LSTM.png)  
### LSTM分解步骤
* 遗忘门：f_t;                    **决定丢弃信息**  
![LSTM_1](https://i.ibb.co/GC8Dhjt/LSTM-1.png)  
* 输入门：i_t,候选记忆单元：~C_t；  **确定更新信息**  
![LSTM_2](https://i.ibb.co/gTkR7tY/LSTM-2.png)  
* 当前时刻记忆单元C_t；            **更新细胞状态**  
![LSTM_3](https://i.ibb.co/GtYBZKD/LSTM-3.png)  
* 输出门：o_t,输出：h_t  
![LSTM_4](https://i.ibb.co/72XpDfy/LSTM-4.png)  

<span id="GRU"></span>
## GRU->LSTM的变种
* 将LSTM网络中的**遗忘门和输入门**用**更新门**来替代
* 也是可以解决RNN网络中的长时依赖问题
* **更新门和重置门**  
![GRU](https://i.ibb.co/4gjb1rc/GRU.png)  
* zt:更新门;用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。
* rt:重置门;控制前一状态有多少信息被写入到当前的候选集 ~h_t上，重置门越小，前一状态的信息被写入的越少
* ~h_t:候选记忆单元
* h_t:当前时刻记忆单元

<span id="Dropout"></span>
## Dropout
我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征  
### dropout可以解决过拟合的原因：
1. 取平均的作用  
> 整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合

2. 减少神经元之间复杂的共适应关系
