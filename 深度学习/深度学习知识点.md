# 深度学习知识点
* [LSTM](#LSTM)
* [GRU](#GRU)
* [Dropout](#Dropout)

<span id="LSTM"></span>
## LSTM
* **输入门、遗忘门和输出门来控制输入值、记忆值和输出值**

<span id="GRU"></span>
## GRU
* LSTM网络的一种效果很好的变体
* 也是可以解决RNN网络中的长时依赖问题
* **更新门和重置门**  
![GRU](https://i.ibb.co/4gjb1rc/GRU.png)  
* zt:更新门;用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。
* rt:重置门;控制前一状态有多少信息被写入到当前的候选集 h~t上，重置门越小，前一状态的信息被写入的越少

<span id="Dropout"></span>
## Dropout
我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征  
### dropout可以解决过拟合的原因：
1. 取平均的作用  
> 整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合

2. 减少神经元之间复杂的共适应关系
