# 深度学习实践

* [参考链接：各种数据集下载](https://mp.weixin.qq.com/s/mq2aCU91zcTe-lkPTiAF2g)
* [tensorflow](#tensorflow)
* [优化算法](#优化算法)

## tensorflow
[链接：tensorflow基础](https://github.com/FangChao1086/machine_learning/tree/master/深度学习/tensorflow/Tensorflow基础.ipynb)  
* 工作原理
  * 是使用**数据流图**（描述的是有向图的数值计算过程）进行数值计算的。
  * 在有向图中，节点表示数学运算，边表示传输多维数据，节点也可以被分配到计算设备上从而进行并行的执行操作
* tf.Interativesession():默认自己就是用户要操作的会话
* tf.Session():没有上面的默认，所以eval()启动计算机室需要志明使用的是哪个会话

### 线性模型与逻辑回归
[参考链接：线性模型与逻辑回归](https://blog.csdn.net/weixin_43824059/article/details/86530652)
### 多层神经网络
[链接：多层神经网络](https://github.com/FangChao1086/machine_learning/blob/master/深度学习/tensorflow/多层神经网络.ipynb)
### 多分类问题与深层神经网络
[链接：多分类问题与深层神经网络_计算图可视化](https://github.com/FangChao1086/machine_learning/blob/master/深度学习/tensorflow/多分类问题及深层神经网络.ipynb)

### 定义深层网络结构
```python
def hidden_layer(layer_input, output_depth, scope='hidden_layer', reuse=None):
    input_depth = layer_input.get_shape()[-1]
    with tf.variable_scope(scope, reuse=reuse):
        # 注意这里的初始化方法是truncated_normal
        w = tf.get_variable(initializer=tf.truncated_normal_initializer(stddev=0.1), shape=(input_depth, output_depth), name='weights')
        # 注意这里用 0.1 对偏置进行初始化
        b = tf.get_variable(initializer=tf.constant_initializer(0.1), shape=(output_depth), name='bias')
        net = tf.matmul(layer_input, w) + b
        
        return net

def DNN(x, output_depths, scope='DNN', reuse=None):
    net = x
    for i, output_depth in enumerate(output_depths):
        net = hidden_layer(net, output_depth, scope='layer%d' % i, reuse=reuse)
        # 注意这里的激活函数
        net = tf.nn.relu(net)
    # 数字分为0, 1, ..., 9 所以这是10分类问题
    # 对应于 one_hot 的标签, 所以这里输出一个 10维 的向量
    net = hidden_layer(net, 10, scope='classification', reuse=reuse)
    
    return net
    
# 使用网络 构造一个4层的神经网络, 它的隐藏节点数分别为: 400, 200, 100, 10
input_ph = tf.placeholder(shape=(None, 784), dtype=tf.float32)
label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)
dnn = DNN(input_ph, [400, 200, 100])
```

## 优化算法
* Momentum
  * `train_op = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)`
  * [链接: Momentum](https://github.com/FangChao1086/machine_learning/blob/master/深度学习/tensorflow/Momentum.ipynb)  
* Adadelta
  * [链接：Adadelta](https://github.com/FangChao1086/machine_learning/blob/master/深度学习/tensorflow/Adadelta.ipynb)
* Adam
  * [链接：Adam](https://github.com/FangChao1086/machine_learning/blob/master/深度学习/tensorflow/Adam.ipynb)
