# 数据挖掘知识点
* [spark数据倾斜](#spark数据倾斜)
* [HDFS](#HDFS)
* [MapReduce](#MapReduce)

<span id="spark数据倾斜"></span>
## spark数据倾斜
**并行处理**的数据集中，**某一部分的数据显著多于其它部分**，从而使得该部分的处理速度成为整个数据集处理的瓶颈   
数据倾斜的原因：在Spark中，**同一个Stage的不同Partition可以并行处理**，而**具有依赖关系的不同Stage之间是串行处理的**。  
* 假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。   

[如何缓解/消除数据倾斜](https://www.cnblogs.com/cssdongl/p/6594298.html)
* 尽量避免数据源的数据倾斜
* 调整并行度分散同一个Task的不同Key

<sapn id="HDFS"></span>
## HDFS
* Hadoop: **开源分布式计算平台**，为用户提供了系统底层细节透明的分布式基础架构
* HDFS: **分布式文件系统**，能对数据进行分布式存储

**hdfs的特点**
* 优点：
  * 支持海量数据的存储
  * 流式数据访问（吞吐量大）
  * 高容错性
* 缺点：
  * 不能做到低延迟数据访问
  * 不适合大量的小文件存储
  * 不支持用户的并行写：同一时间内，只能有一个用户执行写操作。

<span id="MapReduce"></span>
## MapReduce
工作方式：
* MapReduce分为map和reduce，一个map处理一个数据块
* 每个机器上会有多个map，用来处理存储在这个机器上的多个数据块，处理的结果形成（key，value）键值对的形式。
* map处理后的结果由reduce汇总，最后将最终结果进行输出。

> 例子，计算文本中单词出现的个数：
![MapReduce工作模式](https://i.ibb.co/FqVWHK9/Mapreduce.png)

* 最左边是一个文件，分成3个数据块
* 每个map对应一个数据块，对它进行处理，这里就将每个单词出现次数先置为1
* 处理之后，就是shuffle（洗牌）：将key值（这里是单词）相同的放到一起
* 排序sort：按照key进行排序，如四个键值就是按照（h,i,l,y)顺序排好的
* reduce就是将key值出现的次数进行汇总，把value值进行相加,这个结果就是这个单词的次数
* 最后输出文件

数据去重实现步骤：
1. **将value设置为key，并直接输出**。 map输出数据的key为数据，将value设置成空值
2. 在MapReduce流程中，map的输出<key，value>经过shuffle过程聚集成<key，value-list>后会交给reduce
3. reduce阶段不管每个key有多少个value，它直接将输入的key复制为输出的key，并输出（输出中的value被设置成空）

数据排序实现步骤：
1. 在map中将**读入的数据转化成IntWritable型**，然后作为key值输出（value任意）
2. reduce拿到<key，value-list>之后，将输入的key作为value输出，并根据value-list中元素的个数决定输出的次数
3. 输出的key是一个全局变量，它统计当前key的位次

[MapReduce实现数据去重、数据排序、求平均值、多列单行输出](https://blog.csdn.net/qq_38262266/article/details/79182179)
