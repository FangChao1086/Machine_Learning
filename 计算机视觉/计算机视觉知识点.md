# 计算机视觉知识点
* [常用图像数据集](#常用图像数据集)
* [形态学计算_Morphology](#形态学计算_Morphology)
* [R-CNN](#R-CNN)
* [Fast_RCNN](#Fast_RCNN)
* [Faster_RCNN](#Faster_RCNN)
* [Mask_RCNN](#Mask_RCNN)
* [Yolo](#Yolo)
* [SSD](#SSD)

<span id="常用图像数据集"></span>
## 常用图像数据集
* **ImageNet ILSVC 2012**
  * 一千万图像，1000类；标定每张图片中**物体的类别**
* **PASCAL VOC 2007**
  * 一万图像，20类；标定每张图片中**物体的类别和位置**
* **PASCAL VOC 2011**
  * 一万一千图像，21类（20物体类+1其他背景类）

<span id="形态学计算_Morphology"></span>
## 形态学计算_Morphology
**站在图像（白色部分）的角度考虑侵蚀或者扩张**  
* Erosion
![erosion](https://i.ibb.co/2YRpPCL/erosion.png)
* Dilation
![dilation](https://i.ibb.co/Yp2jH6D/dilation.png)
* Opening
  * 先侵蚀，后扩张
  ![opening](https://i.ibb.co/xCsqPZY/opening.png)
* Closing
  * 先扩张，后侵蚀
  ![closing](https://i.ibb.co/SP7M4BZ/closing.png)

[参考链接：基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN](https://www.cnblogs.com/skyfsm/p/6806246.html)
<span id="R-CNN"></span>
## R-CNN
许多候选框（比如两千个）-->resize,CNN-->得到每个候选框的特征-->分类+回归  
![RCNN](https://i.ibb.co/p2qbQ5P/RCNN.png)  

### 步骤：   
* **候选框生成**：在图像中提取约1000-**2000个候选区域（ROI）** (方法**selective search**)  
* **提取特征**：将每个候选区域**缩放至相同大小**，然后对每个区域进行特征提取（采用**CNN**提取4096维）  
* **类别判断**：提取出的特征送入每一类的SVM 分类器，判别是否属于该类
* **位置精修**：对于属于某一特征的候选框，用回归器进一步调整其位置

**训练：使用ImageNet ILSVC 2012进行有监督预训练，而后用PASCAL VOC 2007调优参数，最后在PASCAL VOC 2007上评测**

### 特征提取的算法实现
* **<details><summary>网络架构（选择Alexnet），对每个输入候选框得到一个4096维的特征向量</summary>**
 
  * Alexnet精度：58.5%；
  * VGG精度：66%，选择小卷积核，较小的跨步，计算量是Alexnet的7倍  
  ![Alextnet_layer](https://i.ibb.co/j88jt0q/alexnet.jpg)  
  ![Alextnet_cnn](https://i.ibb.co/QC5fvkQ/Alexnet-cnn.jpg)  
  </details>

* **<details><summary>有监督预训练(迁移学习)：训练一个任务**好的参数**给第二个任务使用</summary>**
 
  * <details><summary>迁移学习的介绍</summary>
 
    * **简单来说，就是把一个任务训练好的参数，拿到另一个任务，作为神经网络的初始参数值**，这样训练出来的参数值比采用随机初始化的方法，精度会有很大提高  
    * 举例：先需要训练一个CNN，用于人脸的年龄识别；当有新任务，人脸识别性别，此时就可以利用已经训练好的年龄CNN模型，去掉最后一层，然后其他层的网络参数直接复制过来，继续进行训练，输出性别即可。  
    </details>  
    
  * **目的：解决物体检测的一个难点，物体标签训练数据少，直接采用随机初始化CNN参数的，目前的训练数据量是远远不够的**
  * 具体实现：在ImagNet上进行有监督的预训练
  
  ![RCNN-pre-train](https://i.ibb.co/kSSYWrf/RCNN-pre-train.jpg)  
  </details>

* **<details><summary>fine-tuning微调阶段（数据：PASCAL VOC)</summary>**

  * 正样本：ground Truth + 与ground Truth 相交IOU > 0.5的候选框
  * 负样本：与ground Truth相交IOU <= 0.5的候选框
  * PASCAL VOC：物体类别 + 物体位置
  * **具体实现：替换上面预训练阶段的CNN模型的最后一层，将其变成N+1个输出神经元（N类+1背景），这一层采用参数随机初始化的方法，其他网络层的参数不变**，进行SGD训练，学习率：0.001  
  
  ![RCNN_fine_tuning](https://i.ibb.co/cLzQJFL/RCNN-fine-tuning.jpg)  
  </details>
  
* 问题：为什么不直接使用微调后的网络进行分类，反而做CNN特征提取，svm分类？
  * 解答：**微调与训练SVM所采用的正负样本阈值不同**，微调阶段正样本标注的IOU阈值设置的宽松(为了增大正样本，防止过拟合)，在实际svm训练过程中对IOU的阈值要求更加严格
  
### SVM训练、位置精修、测试
#### SVM训练
  * cnn的f7层特征被提取出来，我们将为每个类别训练一个SVM分类器
  * 正样本：Ground Truth
  * 负样本：与Ground Truth相交IOU <= 0.3的候选框
    * 负样本过多，采用hard negativa mining的方法在负样本中选取有代表性的负样本

  ![rcnn——pred](https://i.ibb.co/DbQvNw6/rcnn-pred.jpg)  
  
#### 位置精修（Bounding-box regression）
  * 训练样本：判定为本类与真值重叠IOU最大，且IOU > 0.6的候选框 
  * 输入：某类型样本对N个：$\{\(P^{i}, G^{i})\}_ {i=1, ..., N}$ + $P_{i=1,..., N}^{i}$对应的pool5层特征$\phi_{5}\left(P^{i}\right)_ {i=1,..., N}$
  * 输出：回归后的候选框Bounding-box
  * 训练的是四种变换操作的权重向量
  * **<details><summary>回归器的设计：</summary>**
 
    ![Rcnn_regression_box](https://i.ibb.co/g9jjqLp/rcnn-regression-box.png)
    * $P$:候选框
    * $G$:真实框
    * $\hat{G}$:回归后的预测框
    * $P^{i}=\left(P_{x}^{i}, P_{y}^{i}, P_{w}^{i}, P_{h}^{i}\right)$
      * i表示第i个窗口
      * 前两个参数表示中心点坐标，后两个参数表示宽高
    * 定义四种变换$d_{x}(P), d_{y}(P), d_{w}(P), d_{h}(P)$
      * $d_{x}(P), d_{y}(P)$: 通过平移对$x, y$进行变化
      * $d_{* }(P)=w_{* }^{T} \phi_{5}(P)$
        * $d_{* }(P), * :x,y,w,h$是pool5层特征$\phi_{5}(P)$的线性函数
        * $w_{* }^{T}$是需要学习的回归参数
    * 预测框的计算：
      $$\hat{G}_ {x}=P_{w} d_{x}(P)+P_{x}$$  
      $$\hat{G}_ {y}=P_{h} d_{y}(P)+P_{y}$$  
      $$\hat{G}_ {w}=P_{w} \exp \left(d_{w}(P)\right)$$  
      $$\hat{G}_ {h}=P_{h} \exp \left(d_{h}(P)\right)$$  
    * 损失函数Loss$=\operatorname{argmin} \sum_{i=0}^{N}\left(t_{* }^{i}-\hat{w}_ {* }^{T} \phi_{5}\left(P^{i}\right)\right)^{2}+\lambda\|\|\hat{w}_ {* }\|\|^{2}$  
      * $t_{* }^{i}$:真实框与候选框之间的变换关系
      * $\hat{w}_ {* }^{T} \phi_{5}(P^{i})$:预测框与候选框之间的变换关系
    * 回归目标$t_* $由训练输入对$(P,G)$计算得到：
      $$t_{x}=\left(G_{x}-P_{x}\right) / P_{w}$$  
      $$t_{y}=\left(G_{y}-P_{y}\right) / P_{h}$$  
      $$t_{w}=\log \left(G_{w} / P_{w}\right)$$  
      $$t_{h}=\log \left(G_{h} / P_{h}\right)$$</details>   
      
#### 测试
  * 使用selective search在测试图片上提取2000个候选框
  * resize(227 * 227),cnn正向传播，将最后一层特征抽取出来
  * 对于每一个类别，使用这一类训练的SVM分类器对提取的特征进行打分，得到所有候选框对于这一类的打分
  * 非极大值抑制（NMS）去除相交多余的框。canny边缘检测，得到bounding box（然后bounding_boxregression精修）
    * 非极大值抑制先计算出每一个bounding box的得分
    * 然后根据score排序，把score最大的bounding box作为选定的框
    * 计算其他bounding box与当前最大score的box的IOU,去除大于设定值的框，重复。。。
    * **将score小于一定阈值的选定框删除**，得到这一类的结果（然后继续进行下一类）

**<details><summary>框的重叠度评估：IOU</summary>**
 
![IOU](https://i.ibb.co/N7bdvjs/IOU.png)  
算法检测的定位框与人工标注的数据不可能完全匹配，IOU是为了评估两个框的重叠度
</details>

**<details><summary>非极大值抑制：NMS</summary>**
 
![NMS](https://i.ibb.co/z6CxXyn/NMS.png)  
RCNN定位一辆车，可能找出很多框，有些重复过大的需要去除，于是有了NMS；  

假设有6个矩形框，从小到大概率属于车辆的框分别是A、B、C、D、E、F；具体NMS做法：
* 从最大概率的F框开始，判断A-E与F的重叠度IOU是否大于某个阈值
* 假设B、D与F的IOU大于，则丢弃B、D;并标记第一个矩形框F，保留
* 剩下的A、C、E，选择概率大的E，判断A,C与E的重叠度，当重叠度IOU大于阈值，则丢弃A,C；并标记第二个矩形框E，保留</details>

### 总结
* **结果**
  * PASCAL VOC 2010测试集上实现了53.7%的mAP
  * PASCAL VOC 2012测试集上实现了53.3%的mAP
* **创新点**
  * cnn提取图像的特征
  * 采用**大样本有监督的预训练+小样本微调**的方式**解决小样本难以训练**的问题
* **问题**
  * 候选区域重叠，产生特征的重复计算

<span id="Fast_RCNN"></span>
## Fast_RCNN  
一张完整图片-->CNN-->得到每张候选框的特征-->分类+回归  

### 特征提取网络
* **基本结构** 

**RCNN+SPP Net(空间金字塔池化)**  
* SPP Net（ROI pooling）
  * 输入：任意尺寸
  * 输出：固定尺寸
  * 只需要**对原图一次卷积提取特征**，节省了大量的计算时间
* Faster_RCNN将bbox regression放进了神经网络内部

<details><summary>步骤：</summary>  
 
　　1.	在图像中确定约1000-2000个候选框 (使用选择性搜索)  
　　2.	对整张图片输进CNN，得到feature map  
　　3.	找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层  
　　4.	对候选框中提取出的特征，使用分类器判别是否属于一个特定类   
　　5.	对于属于某一特征的候选框，用回归器进一步调整其位置  </details>

### 总结
* **改进**
  * 整张图像直接送入深度网络，解决RCNN产生的特征重复计算的问题
  * 将类别判断与位置精调统一用深度网络实现，不再需要额外存储

<span id="Faster_RCNN"></span>
## Faster_RCNN
改进：抛弃选择性搜索，自动提取候选区域（RPN网络）

<details><summary>步骤：</summary>  
 
　　1.	对整张图片输进CNN，得到feature map  
　　2.	卷积特征输入到RPN，得到候选框的特征信息  
　　3.	对候选框中提取出的特征，使用分类器判别是否属于一个特定类   
　　4.	对于属于某一特征的候选框，用回归器进一步调整其位置  </details>

<span id="Mask_RCNN"></span>
## Mask_RCNN
[Github：Mask_RCNN](https://github.com/matterport/Mask_RCNN)

<span id="Yolo"></span>
## Yolo

<span id="SSD"></span>
## SSD
* 多尺度
* 卷积检测
* 先验框（default box）

### 网络框架
![网络框架](https://i.ibb.co/kyctDkY/ssd-net.jpg)  

### 训练目标
![ssd_训练目标](https://i.ibb.co/x5nMCHn/ssd-loss.jpg)  
$x_{i j}^{p}=1$：第i个框与类别p的第j个真实框是一致的  
损失函数：  
$$L(x, c, l, g)=L_{c o n f}(x, c)+\alpha L_{l o c}(x, l, g)$$  
定位损失函数是预测框与真实框的L2损失：  
$$L_{l o c}(x, l, g)=\frac{1}{2} \sum_{i, j} x_{i j}^{p}\|\|l_{i}-g_{j}^{p}\|\|_ {2}^{2}$$  
分类损失：  
$$L_{conf}(x, c)=-\sum_{i \in P o s}^{N} x_{i j}^{p} \log \left(\hat{c}_ {i}^{p}\right)-\sum_{i \in N e g} \log \left(\hat{c}_ {i}^{0}\right)$$  

