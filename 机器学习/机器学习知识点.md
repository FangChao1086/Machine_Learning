# 机器学习知识点

* [偏差与方差](#偏差与方差)
* [生成模型与判别模型](#生成模型与判别模型)
* [特征选择方法](#特征选择方法)

<span id="偏差与方差"></span>
---
## 偏差与方差
* 偏差：模型**预测的期望值**与**真实值**之间的差距，描述模型的**拟合能力**    
  * 高偏差的解决方案：boosting，复杂化模型，更多特征  
  ![偏差](https://github.com/FangChao1086/Machine_learning/blob/master/素材/1.png#pic_center)  
* 方差：模型**预测的期望值**与**预测值**之间的差平方和，描述模型的**稳定性**   
  * 高方差的解决方案：bagging，模型简化，降维更少特征  
  ![方差](https://github.com/FangChao1086/Machine_learning/blob/master/素材/2.png#pic_center)  
  ![方差](https://github.com/FangChao1086/Machine_learning/blob/master/素材/3.png#pic_center)

<span id="生成模型与判别模型"></span>
---
## 生成模型与判别模型
* 判别模型：**直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型**；不考虑样本的产生模型   
  * 优缺点：能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪   
* 生成模型：**学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型**；P(Y|X)= P(X,Y)/ P(X),其中P(x)是训练数据的概率分布
  * 优缺点：不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面  
  
**由生成模型可以得到判别模型，但由判别模型得不到生成模型**

&emsp;&emsp;生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。

<span id="特征选择方法"></span>
---
## 特征选择方法
* **Filter**：**过滤法**
  * 按照**发散性**或者**相关性**对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征
* **Wrapper**:**包装法**
  * 根据**目标函数**（通常是预测效果评分），每次选择若干特征，或者排除若干特征
* **Embedded**:**嵌入法**
  * 先使用某些**机器学习的算法和模型进行训练**，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣
### Filter
* **1、移除低方差的特征**
  * 当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用；假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，可以认为这个特征作用不大。当100%都是1，那这个特征就没意义
* **2、单变量特征选择（独立的衡量每个特征与响应变量之间的关系）**
  * **2.1、卡方（Chi2）检验**
  * **2.2、皮尔森相关系数**
    * 衡量的是变量之间的线性相关性 结果的取值区间为[-1，1]  
    * 缺陷：只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0
  * **2.3、互信息和最大信息系数**
  * **2.4、距离相关系数**
    * 为了克服Pearson相关系数的弱点  
    * 在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。
  * **2.5、基于模型的特征排序**
    * 这种方法的思路是直接使用你要用的**机器学习算法**，**针对每个单独的特征和响应变量建立预测模型**。假如特征和响应变量之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。
### Wrapper
* **3、递归特征消除**
  * 使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数（绝对值权重小）的特征，再基于新的特征集进行下一轮训练
### Embedded
* **4、使用SelectFromModel选择特征**
  * **4.1、基于L1的特征选择**
    * 使用L1范数作为惩罚项的线性模型(Linear models)会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 feature_selection.SelectFromModel 来选择不为0的系数。特别指出，常用于此目的的稀疏预测模型有 linear_model.Lasso（回归）， linear_model.LogisticRegression 和 svm.LinearSVC（分类）
  * **4.2、随机稀疏模型**
    * 基于L1的稀疏模型的局限在于，当**面对一组互相关的特征时**，它们只会选择其中一项特征。为了减轻该问题的影响可以使用随机化技术，通过_多次重新估计稀疏模型来扰乱设计矩阵_，或通过_多次下采样数据来统计一个给定的回归量被选中的次数
  * **4.3、基于树的特征选择**
    * 基于树的预测模型（见 sklearn.tree 模块，森林见 sklearn.ensemble 模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 sklearn.feature_selection.SelectFromModel）
* **5、将特征选择过程融入pipeline**
  * 在此代码片段中，将　sklearn.svm.LinearSVC 和 sklearn.feature_selection.SelectFromModel 结合来评估特征的重要性，并选择最相关的特征。之后 sklearn.ensemble.RandomForestClassifier 模型使用转换后的输出训练，即只使用被选出的相关特征  
  ![将特征选择过程融入pipeline](https://github.com/FangChao1086/Machine_learning/blob/master/素材/6.png)

