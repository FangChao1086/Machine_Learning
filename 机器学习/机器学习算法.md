# 机器学习算法
* [逻辑斯蒂回归](#逻辑斯蒂回归)
* [支持向量机SVM](#支持向量机SVM)
  * [手撕SVM](#手撕SVM)
  * [软间隔最大化](#软间隔最大化)
  * [合页损失](#合页损失)
  * [核函数](#核函数)
  * [SVM实现多分类](#SVM实现多分类)
* [决策树](#决策树)
* [集成学习](#集成学习)
  * [Adaboost算法](#Adaboost算法)
  * [前向分步算法](#前向分步算法)
  * [梯度提升决策树GBDT](#梯度提升决策树GBDT)
  * [XGBoost算法](#XGBoost算法)
* [聚类](#聚类)
  * [K-means聚类最优k值的选取](#K-means聚类最优k值的选取)
* [隐马尔可夫模型HMM](#隐马尔可夫模型HMM)
<span id="逻辑斯蒂回归"></span>
## 逻辑斯蒂回归
### 逻辑斯蒂回归推导
[参考链接：逻辑回归的推导](https://www.cnblogs.com/lxs0731/p/8573044.html)

<span id="支持向量机SVM"></span>
## 支持向量机SVM
<span id="手撕SVM"></span>
[参考链接：手撕SVM](https://blog.csdn.net/Dominic_S/article/details/83002153)  
支持向量：训练数据集中与分离超平面距离最近的样本点的实例  
### 支持向量机的分类
* 线性可分支持向量机
  * 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即线性可分支持向量机，又称硬间隔支持向量机  
* 线性支持向量机
  * 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称软间隔支持向量机
* 非线性支持向量机
  * 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机
  
**核函数**：将输入从输入空间映射到特征空间，得到的特征向量
### 支持向量机推导
* 线性SVM的推导：
  * 如何根据**间隔最大化**的目标导出 SVM 的**标准问题**  
  * 拉格朗日乘子法对偶问题的求解过程
#### 符号定义
* 训练集 T
  * <a href="https://www.codecogs.com/eqnedit.php?latex=T=\left&space;{&space;(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)&space;\right&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T=\left&space;{&space;(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)&space;\right&space;}" title="T=\left { (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \right }" /></a>  
* 分离超平面 (w,b)
  * <a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;x&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;x&plus;b^*=0" title="w^*\cdot x+b^*=0" /></a>  
  如果使用映射函数，那么分离超平面为
  * <a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;\Phi&space;(x)&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;\Phi&space;(x)&plus;b^*=0" title="w^*\cdot \Phi (x)+b^*=0" /></a>  
  >映射函数 Φ(x) 定义了从输入空间到特征空间的变换，特征空间通常是更高维的，甚至无穷维；方便起见，这里假设 Φ(x) 做的是恒等变换  
* 分类决策函数 f(x)
  * <a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" title="f(x)=\mathrm{sign}(w^*\cdot x+b^*)" /></a>  
### 标准问题推导  
#### 1、从“函数间隔”到“几何间隔”
给定训练集T和超平面(w,b)，定义函数间隔γ^：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i&plus;b)&space;\&space;&=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i&plus;b)&space;\&space;&=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" title="\begin{aligned} \hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i+b) \ &=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" /></a>  
对 w 作规范化，使函数间隔成为几何间隔γ  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})\&space;&=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})\&space;&=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;\end{aligned}" title="\begin{aligned} \gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red} \left\vert | w \vert\right |}}x_i+\frac{b}{{\color{Red} \left\vert | w \vert\right |}})\ &=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red} \left\vert | w \vert\right |}} \end{aligned}" /></a>  
#### 2、最大化几何间隔
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \mathrm{s.t.}\quad, y_i(\frac{w}{{\color{Red} \left\vert | w \vert\right |}}x_i+\frac{b}{{\color{Red} \left\vert | w \vert\right |}}) \geq \gamma,\quad i=1,2,\cdots,N \end{aligned}" /></a>  
也就是：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&\underset{w,b}{\max}&space;\quad{\color{Red}&space;\frac{\hat{\gamma}}{\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)&space;\geq&space;{\color{Red}&space;\hat{\gamma}},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&\underset{w,b}{\max}&space;\quad{\color{Red}&space;\frac{\hat{\gamma}}{\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)&space;\geq&space;{\color{Red}&space;\hat{\gamma}},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &\underset{w,b}{\max} \quad{\color{Red} \frac{\hat{\gamma}}{\left\vert | w \vert\right |}} & \mathrm{s.t.}\quad, y_i(wx_i+b) \geq {\color{Red} \hat{\gamma}},\quad i=1,2,\cdots,N \end{aligned}" /></a>  
函数间隔γ^的取值不会影响最终的超平面(w,b)：取γ^=1；  
>比例改变(ω,b)，超平面不会改变，但函数间隔γ^会成比例改变，因此可以通过等比例改变(ω,b)使函数间隔γ^=1  

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\quad\frac{1}{2}{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}^2&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)-1&space;\geq&space;0,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\quad\frac{1}{2}{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}^2&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)-1&space;\geq&space;0,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\min} } \quad\frac{1}{2}{\color{Red} \left\vert | w \vert\right |}^2 \ & \mathrm{s.t.}\quad, y_i(wx_i+b)-1 \geq 0,\quad i=1,2,\cdots,N \end{aligned}" /></a>  
该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题,可以使用商业 QP 代码完成  
理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况
### 对偶算法推导
1、构建**拉格朗日函数**  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  

2、标准问题是求极小极大问题：  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Red} \underset{w,b}{\min}} {\color{Blue} \underset{\alpha}{\max}} L(w,b,\alpha) \end{aligned}" /></a>  
其对偶问题：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Blue} \underset{\alpha}{\max}} {\color{Red} \underset{w,b}{\min}} L(w,b,\alpha) \end{aligned}" /></a>  

3、求 L 对 (w,b) 的极小  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;&\Rightarrow&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\&space;&\Rightarrow&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;&\Rightarrow&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\&space;&\Rightarrow&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 &\Rightarrow w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;&\Rightarrow&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;&\Rightarrow&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 &\Rightarrow \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}" /></a>  
结果代入L，有：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;\\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;\\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha}) &=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ \\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i+\sum_{i=1}^N \alpha_i\ \\&=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N \alpha_i\ \\&=-\frac{1}{2}w^Tw+\sum_{i=1}^N \alpha_i\ \\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} x_i^Tx_j}+\sum_{i=1}^N \alpha_i \end{aligned}" /></a>

4、求 L 对 α 的极大，即  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &\underset{\alpha}{\max} \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j+\sum_{i=1}^N \alpha_i\ \\& \mathrm{s.t.}\quad \sum_{i=1}^N \alpha_i y_i=0, {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  
> 该问题的对偶问题为：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{\alpha}{\min} } \quad \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j-\sum_{i=1}^N \alpha_i\ \\& \mathrm{s.t.}\quad \sum_{i=1}^N \alpha_i y_i=0, {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  

5、设 α 的解为 α\*，则存在下标j使α_j > 0，可得标准问题的解为：  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;w^*&=\sum_{i=1}^N&space;\alpha_i^*y_ix_i\&space;\\&space;b^*&={\color{Red}&space;y_j}-\sum_{i=1}^N&space;\alpha_i^*y_i(x_i^T{\color{Red}&space;x_j})&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;w^*&=\sum_{i=1}^N&space;\alpha_i^*y_ix_i\&space;\\&space;b^*&={\color{Red}&space;y_j}-\sum_{i=1}^N&space;\alpha_i^*y_i(x_i^T{\color{Red}&space;x_j})&space;\end{aligned}" title="\begin{aligned} w^*&=\sum_{i=1}^N \alpha_i^*y_ix_i\ \\ b^*&={\color{Red} y_j}-\sum_{i=1}^N \alpha_i^*y_i(x_i^T{\color{Red} x_j}) \end{aligned}" /></a>  
>>可得分离超平面及分类决策函数为：  
<a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;x&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;x&plus;b^*=0" title="w^*\cdot x+b^*=0" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" title="f(x)=\mathrm{sign}(w^*\cdot x+b^*)" /></a>  

<span id="软间隔最大化"></span>
### 软间隔最大化
|C值|惩罚|划分错误的点|形状|
|-----|-----|-----|-----|
|大|大|少|瘦|
|小|小|多|胖|

<span id="合页损失"></span>
### 合页损失
* 当样本被正确分类且函数间隔大于1时，合页损失才是0，否则损失是1-y(wx+b)。
* 合页损失函数**不仅要正确分类**，而且**确信度足够高时损失才是0**。也就是说，合页损失函数对学习有更高的要求。

<span id="核函数"></span>
### 核函数
**核函数的种类**
* 线性核函数
* 多项式核函数
  * 多项式形式的核函数具有良好的全局性质
* 径向基核函数（RBF）
  * Gauss径向基函数则是局部性强的核函数
* sigmoid核函数
  * 采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络
  
**核函数的选择**
1. 利用专家的先验知识预先选定核函数  
2. 采用Cross-Validation方法  
  >> 在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数
3. 采用混合核函数方法

<span id="SVM实现多分类"></span>
### SVM实现多分类
[参考链接：SVM实现多分类](https://www.cnblogs.com/CheeseZH/p/5265959.html)  

<span id="决策树"></span>
## 决策树
### 特征选择
* 分类树
  * ID3决策树：信息增益（偏向于特征取值多的特征，而信息增益比则抵消了特征变量的复杂程度）
  * C4.5:信息增益比
  * CART:gini指数最小化
* 回归树
  * CART:平方误差最小化
### CART算法
* 在给定输入随机变量 X 条件下输出随机变量 Y 的**条件概率分布**的学习方法
* 假设决策树是**二叉树**，内部节点特征的取值为“是”和“否”，左分支：是；右分支：否；
  * 这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的条件概率分布。
* CART 决策树既**可以用于分类，也可以用于回归**；

<span id="集成学习"></span>
## 集成学习
* 基本思想：由多个学习器组合成一个性能更好的学习器

### 集成学习的策略
**1. Boosting**
* 从某个基学习器出发，反复学习，得到一系列基学习器，然后组合它们构成一个强学习器
* **串行策略** ：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成
* 代表算法:
  * 提升方法Adaboost
  * 梯度提升树GBDT


Boosting 策略要解决的两个基本问题:
* 每一轮如何**改变数据的权值**或概率分布？
* 如何将弱分类器组合成一个强分类器？

**2. Bagging**
* **并行策略** ：基学习器之间不存在依赖关系，可同时生成
* 代表算法：
  * 随机森林
  * 神经网络中的dropout策略
  
**3. Stacking**


<span id="Adaboost算法"></span>
### Adaboost算法
---
稍后补充

<span id="前向分步算法"></span>
### 前向分步算法
---
#### 加法模型
* 加法模型：  
<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)" title="f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)" /></a>  
  * 基函数：```b(x;γ_m)```
  * 基函数的参数：```γ```
  * 基函数的系数：```β```
* 在给定训练数据和损失函数```L(y,f(x))```的情况下，学习加法模型相当于损失函数的最小化问题  
<a href="https://www.codecogs.com/eqnedit.php?latex={\color{Red}&space;\underset{\beta_m,\gamma_m}{\min}}\sum_{i=1}^N&space;L\left&space;(&space;y_i,{\color{Blue}&space;\sum_{m=1}^M\beta_mb(x;\gamma_m)}&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?{\color{Red}&space;\underset{\beta_m,\gamma_m}{\min}}\sum_{i=1}^N&space;L\left&space;(&space;y_i,{\color{Blue}&space;\sum_{m=1}^M\beta_mb(x;\gamma_m)}&space;\right&space;)" title="{\color{Red} \underset{\beta_m,\gamma_m}{\min}}\sum_{i=1}^N L\left ( y_i,{\color{Blue} \sum_{m=1}^M\beta_mb(x;\gamma_m)} \right )" /></a>  
#### 算法描述
思想：从前向后，每一步只学习一个基函数及其系数，逐步优化目标函数  
* 输入：训练集```T={(x1,y1),..,(xN,yN)}```，损失函数```L(y,f(x))```，基函数集```{b(x;γ)}```
* 输出：加法模型```f(x)```  

1. 初始化```f_0(x)=0```    
2. 对```m=1,2,..,M```
  * 极小化损失函数，得到```(β_m,γ_m)```  
<a href="https://www.codecogs.com/eqnedit.php?latex=(\beta_m,\gamma_m)=\arg\underset{\beta,\gamma}{\min}\sum_{i=1}^NL\left&space;(&space;y_i,{\color{Red}&space;f_{m-1}(x_i)&plus;\beta&space;b(x_i;\gamma)}&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\beta_m,\gamma_m)=\arg\underset{\beta,\gamma}{\min}\sum_{i=1}^NL\left&space;(&space;y_i,{\color{Red}&space;f_{m-1}(x_i)&plus;\beta&space;b(x_i;\gamma)}&space;\right&space;)" title="(\beta_m,\gamma_m)=\arg\underset{\beta,\gamma}{\min}\sum_{i=1}^NL\left ( y_i,{\color{Red} f_{m-1}(x_i)+\beta b(x_i;\gamma)} \right )" /></a>  
  * 更新模型 ```f_m(x)```  
  <a href="https://www.codecogs.com/eqnedit.php?latex=f_m(x)={\color{Red}&space;f_{m-1}(x)&plus;\beta&space;b(x;\gamma)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_m(x)={\color{Red}&space;f_{m-1}(x)&plus;\beta&space;b(x;\gamma)}" title="f_m(x)={\color{Red} f_{m-1}(x)+\beta b(x;\gamma)}" /></a>  
3. 得到加法模型  
<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=f_M(x)={\color{Red}&space;\sum_{m=1}^M}{\color{Blue}&space;\beta_m}b(x;\gamma_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=f_M(x)={\color{Red}&space;\sum_{m=1}^M}{\color{Blue}&space;\beta_m}b(x;\gamma_m)" title="f(x)=f_M(x)={\color{Red} \sum_{m=1}^M}{\color{Blue} \beta_m}b(x;\gamma_m)" /></a>  
* 前向分步算法将同时求解```m=1,2,..,M```所有参数```(β_m,γ_m)```的问题简化为逐次求解各```(β_m,γ_m)```的优化问题——思想上有点像梯度下降  
#### 前向分步算法与 AdaBoost
* AdaBoost 算法是前向分步算法的特例。
* 此时，基函数为基分类器，损失函数为指数函数```L(y,f(x)) = exp(-y*f(x))```

<span id="梯度提升决策树GBDT"></span>
### 梯度提升决策树-GBDT
---
* GBDT 是以**决策树**为基学习器、采用 Boosting 策略的一种集成学习模型
* **与提升树的区别**：残差的计算不同，提升树使用的是真正的残差，梯度提升树用当前模型的负梯度来拟合残差。  
#### 提升树 Boosting Tree
---
* 以**决策树**为基学习器，对分类问题使用二叉分类树，回归问题使用二叉回归树。
* 解决回归问题时，通过不断拟合残差得到新的树。
* 提升树模型可表示为**决策树的加法模型**：  
<a href="https://www.codecogs.com/eqnedit.php?latex=f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" title="f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" /></a>  
* 首先初始化提升树```f_0(x)=0```，则第 m 步的模型为  
<a href="https://www.codecogs.com/eqnedit.php?latex=f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)" title="f_m(x)=f_{m-1}(x)+T(x;\Theta_m)" /></a>  
* 然后通过最小化损失函数决定下一个决策树的参数  
<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^NL(y_i,{\color{Red}&space;f_{m-1}(x_i)&plus;T(x_i;\Theta_m)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^NL(y_i,{\color{Red}&space;f_{m-1}(x_i)&plus;T(x_i;\Theta_m)})" title="\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^NL(y_i,{\color{Red} f_{m-1}(x_i)+T(x_i;\Theta_m)})" /></a>  
* 对于二分类问题，提升树算法只需要将[AdaBoost算法](#Adaboost算法)中的基学习器限制为二叉分类树即可  

**提升树算法描述**
在回归问题中，新的树是通过不断**拟合残差**（residual）得到的。
* 输入：训练集```T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R```    
* 输出：回归提升树```f_M(x)```  
1. 初始化```f_0(x)=0```  
2. 对```m=1,2,..,M```  
  * 计算**残差**  
  <a href="https://www.codecogs.com/eqnedit.php?latex={\color{Red}&space;r_{m,i}}=y_i-f_{m-1}(x_i),\quad&space;i=1,2,..,N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?{\color{Red}&space;r_{m,i}}=y_i-f_{m-1}(x_i),\quad&space;i=1,2,..,N" title="{\color{Red} r_{m,i}}=y_i-f_{m-1}(x_i),\quad i=1,2,..,N" /></a>  
  * **拟合残差**学习下一个回归树的参数  
  <a href="https://www.codecogs.com/eqnedit.php?latex=\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N&space;L({\color{Red}&space;r_{m,i}},{\color{Blue}&space;T(x_i;\Theta_m)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N&space;L({\color{Red}&space;r_{m,i}},{\color{Blue}&space;T(x_i;\Theta_m)})" title="\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N L({\color{Red} r_{m,i}},{\color{Blue} T(x_i;\Theta_m)})" /></a>  
  * 更新```f_m(x)```  
  <a href="https://www.codecogs.com/eqnedit.php?latex=f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_m(x)=f_{m-1}(x)&plus;T(x;\Theta_m)" title="f_m(x)=f_{m-1}(x)+T(x;\Theta_m)" /></a>  
3. 得到回归提升树  
<a href="https://www.codecogs.com/eqnedit.php?latex=f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" title="f_M(x)=\sum_{m=1}^MT(x;\Theta_m)" /></a>  

**梯度提升算法（GB）**
* 当损失函数为平方损失或指数损失时，每一步的优化是很直观的；但对于一般的损失函数而言，不太容易——梯度提升正是针对这一问题提出的算法；
* 梯度提升是梯度下降的近似方法，其关键是利用**损失函数的负梯度**作为**残差的近似值**，来拟合下一个决策树。

#### GBDT算法描述
* 输入：训练集```T={(x1,y1),..,(xN,yN)}, xi ∈ R^n, yi ∈ R```；损失函数```L(y,f(x))```；
* 输出：回归树```f_M(x)```
1. 初始化回归树  
<a href="https://www.codecogs.com/eqnedit.php?latex=f_0(x)={\color{Red}&space;\arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_0(x)={\color{Red}&space;\arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" title="f_0(x)={\color{Red} \arg\underset{c}{\min}}\sum_{i=1}^NL(y_i,c)" /></a>  
2. 对```m=1,2,..,M```
  * 对```i=1,2,..,N```，计算残差/负梯度  
  <a href="https://www.codecogs.com/eqnedit.php?latex=r_{m,i}=-\frac{\partial&space;L(y_i,{\color{Red}&space;f_{m-1}(x_i)}))}{\partial&space;{\color{Red}&space;f_{m-1}(x_i)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?r_{m,i}=-\frac{\partial&space;L(y_i,{\color{Red}&space;f_{m-1}(x_i)}))}{\partial&space;{\color{Red}&space;f_{m-1}(x_i)}}" title="r_{m,i}=-\frac{\partial L(y_i,{\color{Red} f_{m-1}(x_i)}))}{\partial {\color{Red} f_{m-1}(x_i)}}" /></a>  
  * 对```r_mi```拟合一个回归树，得到第 m 棵树的叶节点区域  
  
  <a href="https://www.codecogs.com/eqnedit.php?latex=R_{m,j},\quad&space;j=1,2,..,J" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{m,j},\quad&space;j=1,2,..,J" title="R_{m,j},\quad j=1,2,..,J" /></a>  
  
  * 对```j=1,2,..,J```，计算  
  
  <a href="https://www.codecogs.com/eqnedit.php?latex=c_{m,j}={\color{Red}&space;\arg\underset{c}{\min}}\sum_{x_i\in&space;R_{m,j}}L(y_i,{\color{Blue}&space;f_{m-1}(x_i)&plus;c})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?c_{m,j}={\color{Red}&space;\arg\underset{c}{\min}}\sum_{x_i\in&space;R_{m,j}}L(y_i,{\color{Blue}&space;f_{m-1}(x_i)&plus;c})" title="c_{m,j}={\color{Red} \arg\underset{c}{\min}}\sum_{x_i\in R_{m,j}}L(y_i,{\color{Blue} f_{m-1}(x_i)+c})" /></a>  
  
  * 更新回归树  
  
  <a href="https://www.codecogs.com/eqnedit.php?latex=f_m(x)=f_{m-1}&plus;\sum_{j=1}^J&space;c_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_m(x)=f_{m-1}&plus;\sum_{j=1}^J&space;c_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" title="f_m(x)=f_{m-1}+\sum_{j=1}^J c_{m,j}{\color{Blue} I(x\in R_{m,j})}" /></a>  
  
3. 得到回归树  
<a href="https://www.codecogs.com/eqnedit.php?latex=f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue}&space;I(x\in&space;R_{m,j})}" title="f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue} I(x\in R_{m,j})}" /></a>  

* 说明： 
  * 算法第 1 步初始化，估计使损失函数最小的常数值，得到一棵只有一个根节点的树
  * 第 2(i) 步计算损失函数的负梯度，将其作为残差的估计 
    * 对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似
  * 第 2(ii) 步估计回归树的节点区域，以拟合残差的近似值
  * 第 2(iii) 步利用线性搜索估计叶节点区域的值，使损失函数最小化

### XGBoost算法
详细推导：  
* 目标函数  
![xgb_1](https://i.ibb.co/d5DWZFR/xgboost-1.png) ![xgb_2](https://i.ibb.co/tYbBf5F/xgboost-2.png)  
  * 模型的复杂度：**叶子节点的数目**和**叶子节点输出Score的L2模的平方**  
  * 模型参数：1、树的结构	2、叶节点的分数（权重）  
* 加法训练：先优化一棵树，再优化另一棵树，依次优化完K棵树  
![xgb_3](https://i.ibb.co/ZmYtsVJ/xgboost-3.png)  
  * 第t步添加了一棵最优CART树，这棵树是从前t-1棵树产生：  
  ![xgb_4](https://i.ibb.co/kXmWcZR/xgboost-4.png)  
  * 由泰勒公式：  
  ![xgb_5](https://i.ibb.co/7JdHmsq/xgboost-5.png)  
  * 此时的目标函数：  
  ![xgb_6](https://i.ibb.co/bNHcmYw/xgboost-6.png)  
  * 第t棵树是优化后的树，即t为变量可优化，上式中的中括号以内是前t-1棵树优化完旧已经确定的常量，因此优化函数为：  
  ![xgb_7](https://i.ibb.co/9wXYyLr/xgboost-7.png)  
  ![xgb_8](https://i.ibb.co/W6frr7r/xgboost-8.png)![xgb_9](https://i.ibb.co/xHpk1kh/xgboost-9.png)  
* 求最优值：  
![xgb_10](https://i.ibb.co/1bHW62L/xgboost-10.png)![xgb_11](https://i.ibb.co/7Jpx2Qy/xgboost-11.png)    
* 上式是一个二次式，可求出叶子节点的最佳值和目标函数的值：
  ![xgb_12](https://i.ibb.co/mJfX8bR/xgboost-12.png)  

求树结构：  
![xgb_13](https://i.ibb.co/SRNf641/xgboost-13.png)  
切分点：  
![xgb_14](https://i.ibb.co/q1GmMBJ/xgboost-14.png) **T(split)-T(nosplit)=1**  

<span id="聚类"></span>
## 聚类
<span id="K-means聚类最优k值的选取"></span>
## K-means聚类最优k值的选取
* 手肘法
  * 思想：利用SSE;**当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减**，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而**这个肘部对应的k值就是数据的真实聚类数**
* 轮廓系数法
  * 思想：平均轮廓系数最大的k便是最佳聚类数  
  
[参考链接：K-means聚类最优k值的方法](https://blog.csdn.net/qq_15738501/article/details/79036255)

<span id="隐马尔可夫模型HMM"></span>
## 隐马尔可夫模型HMM
隐藏的马尔可夫链随机生成**状态序列**，每个状态随机生成一个观测，由此产生**观测序列**  
* 初始状态概率向量pai
* 状态转移概率矩阵A
* 观测概率矩阵B  

其中pai和A决定状态序列，B决定观测序列  
### 维特比算法
用动态规划解隐马尔可夫模型**预测问题**，求最优路径（概率最大路径），也就是概率最大状态序列
* 输入：模型与观测
* 输出：最优路径
