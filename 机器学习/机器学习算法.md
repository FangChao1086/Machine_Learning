# 机器学习算法
* [逻辑斯蒂回归](#逻辑斯蒂回归)
* [支持向量机](#支持向量机)
* [决策树](#决策树)
* [集成学习](#集成学习)
  * [Adaboost算法](#[Adaboost算法)
  * [前向分步算法](#前向分步算法)
  * [梯度提升决策树](#梯度提升决策树)

<span id="逻辑斯蒂回归"></span>
## 逻辑斯蒂回归
### 逻辑斯蒂回归推导
1、逻辑斯蒂回归的定义：  
![log](https://github.com/FangChao1086/Machine_learning/blob/master/素材/4.PNG#pic_center)  
2、负对数函数作为损失函数：
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w)&=-\log\left&space;(&space;\prod_{i=1}^N&space;[{\color{Red}&space;\sigma(x_i)}]^{{\color{Blue}&space;y_i}}&space;[{\color{Red}&space;1-&space;\sigma(x_i)}]^{{\color{Blue}&space;1-y_i}}&space;\right&space;)\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\sigma(x_i)&plus;(1-y_i)\log(1-\sigma(x_i))&space;\right&space;]\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}&plus;\log(1-\sigma(x_i))&space;\right&space;]&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w)&=-\log\left&space;(&space;\prod_{i=1}^N&space;[{\color{Red}&space;\sigma(x_i)}]^{{\color{Blue}&space;y_i}}&space;[{\color{Red}&space;1-&space;\sigma(x_i)}]^{{\color{Blue}&space;1-y_i}}&space;\right&space;)\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\sigma(x_i)&plus;(1-y_i)\log(1-\sigma(x_i))&space;\right&space;]\&space;&=-\sum_{i=1}^N&space;\left&space;[&space;y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}&plus;\log(1-\sigma(x_i))&space;\right&space;]&space;\end{aligned}" title="\begin{aligned} L(w)&=-\log\left ( \prod_{i=1}^N [{\color{Red} \sigma(x_i)}]^{{\color{Blue} y_i}} [{\color{Red} 1- \sigma(x_i)}]^{{\color{Blue} 1-y_i}} \right )\ &=-\sum_{i=1}^N \left [ y_i\log\sigma(x_i)+(1-y_i)\log(1-\sigma(x_i)) \right ]\ &=-\sum_{i=1}^N \left [ y_i\log\frac{\sigma(x_i)}{1-\sigma(x_i)}+\log(1-\sigma(x_i)) \right ] \end{aligned}" /></a>  
进一步代入 σ(x) 有：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w)&=-\sum_{i=1}^N&space;\left&space;[&space;{\color{Blue}&space;y_i}(w{\color{Red}&space;x_i})-\log(1&plus;\exp(w{\color{Red}&space;x_i}))&space;\right&space;]&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w)&=-\sum_{i=1}^N&space;\left&space;[&space;{\color{Blue}&space;y_i}(w{\color{Red}&space;x_i})-\log(1&plus;\exp(w{\color{Red}&space;x_i}))&space;\right&space;]&space;\end{aligned}" title="\begin{aligned} L(w)&=-\sum_{i=1}^N \left [ {\color{Blue} y_i}(w{\color{Red} x_i})-\log(1+\exp(w{\color{Red} x_i})) \right ] \end{aligned}" /></a>  
3、求梯度  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\frac{\partial&space;L(w)}{\partial&space;w}&=-\sum_{i=1}^N&space;\left&space;[&space;y_ix_i-\frac{\exp(wx_i)}{1&plus;\exp(wx_i)}x_i&space;\right&space;]\&space;&=\sum_{i=1}^N&space;[\sigma&space;(x_i)-y_i]x_i&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\frac{\partial&space;L(w)}{\partial&space;w}&=-\sum_{i=1}^N&space;\left&space;[&space;y_ix_i-\frac{\exp(wx_i)}{1&plus;\exp(wx_i)}x_i&space;\right&space;]\&space;&=\sum_{i=1}^N&space;[\sigma&space;(x_i)-y_i]x_i&space;\end{aligned}" title="\begin{aligned} \frac{\partial L(w)}{\partial w}&=-\sum_{i=1}^N \left [ y_ix_i-\frac{\exp(wx_i)}{1+\exp(wx_i)}x_i \right ]\ &=\sum_{i=1}^N [\sigma (x_i)-y_i]x_i \end{aligned}" /></a>  
最后使用梯度下降法求解参数  

<span id="支持向量机"></span>
## 支持向量机
支持向量：训练数据集中与分离超平面距离最近的样本点的实例  
### 支持向量机的分类
* 线性可分支持向量机
  * 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即线性可分支持向量机，又称硬间隔支持向量机  
* 线性支持向量机
  * 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称软间隔支持向量机
* 非线性支持向量机
  * 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机
  
**核函数**：将输入从输入空间映射到特征空间，得到的特征向量
### 支持向量机推导
* 线性SVM的推导：
  * 如何根据**间隔最大化**的目标导出 SVM 的**标准问题**  
  * 拉格朗日乘子法对偶问题的求解过程
#### 符号定义
* 训练集 T
  * <a href="https://www.codecogs.com/eqnedit.php?latex=T=\left&space;{&space;(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)&space;\right&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T=\left&space;{&space;(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)&space;\right&space;}" title="T=\left { (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \right }" /></a>  
* 分离超平面 (w,b)
  * <a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;x&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;x&plus;b^*=0" title="w^*\cdot x+b^*=0" /></a>  
  如果使用映射函数，那么分离超平面为
  * <a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;\Phi&space;(x)&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;\Phi&space;(x)&plus;b^*=0" title="w^*\cdot \Phi (x)+b^*=0" /></a>  
  >映射函数 Φ(x) 定义了从输入空间到特征空间的变换，特征空间通常是更高维的，甚至无穷维；方便起见，这里假设 Φ(x) 做的是恒等变换  
* 分类决策函数 f(x)
  * <a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" title="f(x)=\mathrm{sign}(w^*\cdot x+b^*)" /></a>  
### 标准问题推导  
#### 1、从“函数间隔”到“几何间隔”
给定训练集T和超平面(w,b)，定义函数间隔γ^：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i&plus;b)&space;\&space;&=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i&plus;b)&space;\&space;&=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" title="\begin{aligned} \hat{\gamma}&=\underset{i=1,\cdots,N}{\min},y_i(wx_i+b) \ &=\underset{i=1,\cdots,N}{\min},\hat{\gamma}_i\end{aligned}" /></a>  
对 w 作规范化，使函数间隔成为几何间隔γ  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})\&space;&=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})\&space;&=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;\end{aligned}" title="\begin{aligned} \gamma&=\underset{i=1,\cdots,N}{\min},y_i(\frac{w}{{\color{Red} \left\vert | w \vert\right |}}x_i+\frac{b}{{\color{Red} \left\vert | w \vert\right |}})\ &=\underset{i=1,\cdots,N}{\min},\frac{\gamma_i}{{\color{Red} \left\vert | w \vert\right |}} \end{aligned}" /></a>  
#### 2、最大化几何间隔
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\max}}&space;\quad\gamma&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(\frac{w}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}}x_i&plus;\frac{b}{{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}})&space;\geq&space;\gamma,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\max}} \quad\gamma \ & \mathrm{s.t.}\quad, y_i(\frac{w}{{\color{Red} \left\vert | w \vert\right |}}x_i+\frac{b}{{\color{Red} \left\vert | w \vert\right |}}) \geq \gamma,\quad i=1,2,\cdots,N \end{aligned}" /></a>  
也就是：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&\underset{w,b}{\max}&space;\quad{\color{Red}&space;\frac{\hat{\gamma}}{\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)&space;\geq&space;{\color{Red}&space;\hat{\gamma}},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&\underset{w,b}{\max}&space;\quad{\color{Red}&space;\frac{\hat{\gamma}}{\left\vert&space;|&space;w&space;\vert\right&space;|}}&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)&space;\geq&space;{\color{Red}&space;\hat{\gamma}},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &\underset{w,b}{\max} \quad{\color{Red} \frac{\hat{\gamma}}{\left\vert | w \vert\right |}} & \mathrm{s.t.}\quad, y_i(wx_i+b) \geq {\color{Red} \hat{\gamma}},\quad i=1,2,\cdots,N \end{aligned}" /></a>  
函数间隔γ^的取值不会影响最终的超平面(w,b)：取γ^=1；  
>比例改变(ω,b)，超平面不会改变，但函数间隔γ^会成比例改变，因此可以通过等比例改变(ω,b)使函数间隔γ^=1  

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\quad\frac{1}{2}{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}^2&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)-1&space;\geq&space;0,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{w,b}{\min}&space;}&space;\quad\frac{1}{2}{\color{Red}&space;\left\vert&space;|&space;w&space;\vert\right&space;|}^2&space;\&space;&&space;\mathrm{s.t.}\quad,&space;y_i(wx_i&plus;b)-1&space;\geq&space;0,\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{w,b}{\min} } \quad\frac{1}{2}{\color{Red} \left\vert | w \vert\right |}^2 \ & \mathrm{s.t.}\quad, y_i(wx_i+b)-1 \geq 0,\quad i=1,2,\cdots,N \end{aligned}" /></a>  
该约束最优化问题即为**线性支持向量机**的标准问题——这是一个**凸二次优化**问题,可以使用商业 QP 代码完成  
理论上，线性 SVM 的问题已经解决了；但在高等数学中，**带约束的最优化问题**还可以用另一种方法求解——**拉格朗日乘子法**。该方法的优点一是更容易求解，而是自然引入**核函数**，进而推广到非线性的情况
### 对偶算法推导
1、构建**拉格朗日函数**  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;&{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha})=&\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ &{\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  

2、标准问题是求极小极大问题：  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Red} \underset{w,b}{\min}} {\color{Blue} \underset{\alpha}{\max}} L(w,b,\alpha) \end{aligned}" /></a>  
其对偶问题：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;L(w,b,\alpha)&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;{\color{Blue}&space;\underset{\alpha}{\max}}&space;{\color{Red}&space;\underset{w,b}{\min}}&space;L(w,b,\alpha)&space;\end{aligned}" title="\begin{aligned} {\color{Blue} \underset{\alpha}{\max}} {\color{Red} \underset{w,b}{\min}} L(w,b,\alpha) \end{aligned}" /></a>  

3、求 L 对 (w,b) 的极小  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;&\Rightarrow&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\&space;&\Rightarrow&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;w}=0&space;&\Rightarrow&space;w-\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}=0\&space;&\Rightarrow&space;w=\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i&space;x_i}&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial w}=0 &\Rightarrow w-\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i}=0\ &\Rightarrow w=\sum_{i=1}^N {\color{Red} \alpha_i y_i x_i} \end{aligned}" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;&\Rightarrow&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;\mathrm{set}\quad&space;\frac{\partial&space;L}{\partial&space;b}=0&space;&\Rightarrow&space;\sum_{i=1}^N&space;{\color{Red}&space;\alpha_i&space;y_i}=0&space;\end{aligned}" title="\begin{aligned} \mathrm{set}\quad \frac{\partial L}{\partial b}=0 &\Rightarrow \sum_{i=1}^N {\color{Red} \alpha_i y_i}=0 \end{aligned}" /></a>  
结果代入L，有：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;\\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;L(w,b,{\color{Red}&space;\alpha})&space;&=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red}&space;\alpha_i}[y_i(w^Tx_i&plus;b)-1]\&space;\\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N&space;\alpha_iy_ix_i-b\sum_{i=1}^N&space;\alpha_iy_i&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=\frac{1}{2}w^Tw-w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}w^Tw&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;{\color{Red}&space;x_i^Tx_j}&plus;\sum_{i=1}^N&space;\alpha_i&space;\end{aligned}" title="\begin{aligned} L(w,b,{\color{Red} \alpha}) &=\frac{1}{2}w^Tw-\sum_{i=1}^N{\color{Red} \alpha_i}[y_i(w^Tx_i+b)-1]\ \\&=\frac{1}{2}w^Tw-w^T\sum_{i=1}^N \alpha_iy_ix_i-b\sum_{i=1}^N \alpha_iy_i+\sum_{i=1}^N \alpha_i\ \\&=\frac{1}{2}w^Tw-w^Tw+\sum_{i=1}^N \alpha_i\ \\&=-\frac{1}{2}w^Tw+\sum_{i=1}^N \alpha_i\ \\&=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot {\color{Red} x_i^Tx_j}+\sum_{i=1}^N \alpha_i \end{aligned}" /></a>

4、求 L 对 α 的极大，即  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&\underset{\alpha}{\max}&space;\quad&space;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j&plus;\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &\underset{\alpha}{\max} \quad -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j+\sum_{i=1}^N \alpha_i\ \\& \mathrm{s.t.}\quad \sum_{i=1}^N \alpha_i y_i=0, {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  
> 该问题的对偶问题为：  
<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;&{\color{Red}&space;\underset{\alpha}{\min}&space;}&space;\quad&space;\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N&space;\alpha_i\alpha_j\cdot&space;y_iy_j\cdot&space;x_i^Tx_j-\sum_{i=1}^N&space;\alpha_i\&space;\\&&space;\mathrm{s.t.}\quad&space;\sum_{i=1}^N&space;\alpha_i&space;y_i=0,&space;{\color{Red}&space;\alpha_i&space;\geq&space;0},\quad&space;i=1,2,\cdots,N&space;\end{aligned}" title="\begin{aligned} &{\color{Red} \underset{\alpha}{\min} } \quad \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_j\cdot y_iy_j\cdot x_i^Tx_j-\sum_{i=1}^N \alpha_i\ \\& \mathrm{s.t.}\quad \sum_{i=1}^N \alpha_i y_i=0, {\color{Red} \alpha_i \geq 0},\quad i=1,2,\cdots,N \end{aligned}" /></a>  

5、设 α 的解为 α\*，则存在下标j使α_j > 0，可得标准问题的解为：  
><a href="https://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&space;w^*&=\sum_{i=1}^N&space;\alpha_i^*y_ix_i\&space;\\&space;b^*&={\color{Red}&space;y_j}-\sum_{i=1}^N&space;\alpha_i^*y_i(x_i^T{\color{Red}&space;x_j})&space;\end{aligned}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{aligned}&space;w^*&=\sum_{i=1}^N&space;\alpha_i^*y_ix_i\&space;\\&space;b^*&={\color{Red}&space;y_j}-\sum_{i=1}^N&space;\alpha_i^*y_i(x_i^T{\color{Red}&space;x_j})&space;\end{aligned}" title="\begin{aligned} w^*&=\sum_{i=1}^N \alpha_i^*y_ix_i\ \\ b^*&={\color{Red} y_j}-\sum_{i=1}^N \alpha_i^*y_i(x_i^T{\color{Red} x_j}) \end{aligned}" /></a>  
>>可得分离超平面及分类决策函数为：  
<a href="https://www.codecogs.com/eqnedit.php?latex=w^*\cdot&space;x&plus;b^*=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w^*\cdot&space;x&plus;b^*=0" title="w^*\cdot x+b^*=0" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\mathrm{sign}(w^*\cdot&space;x&plus;b^*)" title="f(x)=\mathrm{sign}(w^*\cdot x+b^*)" /></a>  

<span id="决策树"></span>
## 决策树
### 特征选择
* 分类树
  * ID3决策树：信息增益
  * C4.5:信息增益比
  * CART:gini指数最小化
* 回归树
  * CART:平方误差最小化
### CART算法
* 在给定输入随机变量 X 条件下输出随机变量 Y 的**条件概率分布**的学习方法
* 假设决策树是**二叉树**，内部节点特征的取值为“是”和“否”，左分支：是；右分支：否；
  * 这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的条件概率分布。
* CART 决策树既**可以用于分类，也可以用于回归**；

<span =id="集成学习"></span>
## 集成学习
* 基本思想：由多个学习器组合成一个性能更好的学习器

### 集成学习的策略
**1. Boosting**
* 从某个基学习器出发，反复学习，得到一系列基学习器，然后组合它们构成一个强学习器
* **串行策略** ：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成
* 代表算法:
  * 提升方法Adaboost
  * 梯度提升树GBDT


Boosting 策略要解决的两个基本问题:
* 每一轮如何**改变数据的权值**或概率分布？
* 如何将弱分类器组合成一个强分类器？

**2. Bagging**
* **并行策略** ：基学习器之间不存在依赖关系，可同时生成
* 代表算法：
  * 随机森林
  * 神经网络中的dropout策略
  
**3. Stacking**


<span id="Adaboost算法"></span>
### Adaboost算法
---
稍后补充

<span id="前向分步算法"></span>
### 前向分步算法
---
#### 加法模型
* 加法模型：
<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=\sum_{m=1}^M\beta_m,b(x;\gamma_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=\sum_{m=1}^M\beta_m,b(x;\gamma_m)" title="f(x)=\sum_{m=1}^M\beta_m,b(x;\gamma_m)" /></a>  


