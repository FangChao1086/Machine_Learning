# 数据挖掘知识点
* [spark数据倾斜](#spark数据倾斜)

<span id="spark数据倾斜"></span>
## spark数据倾斜
**并行处理**的数据集中，**某一部分的数据显著多于其它部分**，从而使得该部分的处理速度成为整个数据集处理的瓶颈   
数据倾斜的原因：在Spark中，**同一个Stage的不同Partition可以并行处理**，而**具有依赖关系的不同Stage之间是串行处理的**。  
* 假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。   

[如何缓解/消除数据倾斜](https://www.cnblogs.com/cssdongl/p/6594298.html)
* 尽量避免数据源的数据倾斜
* 调整并行度分散同一个Task的不同Key

