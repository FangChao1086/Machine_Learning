# 数据挖掘知识点
* [spark数据倾斜](#spark数据倾斜)
* [MapReduce](#MapReduce)

<span id="spark数据倾斜"></span>
## spark数据倾斜
**并行处理**的数据集中，**某一部分的数据显著多于其它部分**，从而使得该部分的处理速度成为整个数据集处理的瓶颈   
数据倾斜的原因：在Spark中，**同一个Stage的不同Partition可以并行处理**，而**具有依赖关系的不同Stage之间是串行处理的**。  
* 假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。   

[如何缓解/消除数据倾斜](https://www.cnblogs.com/cssdongl/p/6594298.html)
* 尽量避免数据源的数据倾斜
* 调整并行度分散同一个Task的不同Key

<span id="MapReduce"></span>
## MapReduce
* Hadoop: **开源分布式计算平台**，为用户提供了系统底层细节透明的分布式基础架构
* HDFS: **分布式文件系统**，能对数据进行分布式存储

工作方式：
* MapReduce分为map和reduce，一个map处理一个数据块
* 每个机器上会有多个map，用来处理存储在这个机器上的多个数据块，处理的结果形成（key，value）键值对的形式。
* map处理后的结果由reduce汇总，最后将最终结果进行输出。

> 例子，计算文本中单词出现的个数：
![MapReduce工作模式](https://i.ibb.co/FqVWHK9/Mapreduce.png)

* 最左边是一个文件，分成3个数据块
* 每个map对应一个数据块，对它进行处理，这里就将每个单词出现次数先置为1
* 处理之后，就是shuffle（洗牌）：将key值（这里是单词）相同的放到一起
* 排序sort：按照key进行排序，如四个键值就是按照（h,i,l,y)顺序排好的
* reduce就是将key值出现的次数进行汇总，把value值进行相加,这个结果就是这个单词的次数
* 最后输出文件
